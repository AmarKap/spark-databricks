{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Spark SQL in Python\n",
                "\n",
                "## Pyspark SQL\n",
                "\n",
                "- Spark provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine.\n",
                "- The dataframe is a fundamental data abstraction in Spark.\n",
                "- A Spark DataFrame is a distributed collection of data organized into named columns and is conceptually equivalent to a table in a relational database, also called, simply, “tabular” data.\n",
                "- Distributed means Spark can split this dataset into parts then store each part on a\n",
                "different server. \n",
                "- Spark SQL table allows us to take the data that is in a dataframe, namely, a distributed collection of rows having named columns, and treat it as a single table, and fetch data from it using an SQL query.\n",
                "- We often use an instance of a SparkSession object. By convention this is provided in a variable called \"spark\". Some implementations of Spark, such as Pyspark Shell, automatically provide an instance of a SparkSession.\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "a91922fc-426c-435e-aaf8-9292f9714d42"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# General Syntax\r\n",
                "df = spark.read.csv(filename)\r\n",
                "\r\n",
                "# General Synatx when first line is column\r\n",
                "df = spark.read.csv(filename, header=True)\r\n",
                "\r\n",
                "# Spark table from dataframe\r\n",
                "df.createOrReplaceTempView(\"schedule\")\r\n",
                "\r\n",
                "# Run a sql\r\n",
                "spark.sql(\"SELECT * FROM schedule WHERE station = 'San Jose'\").show()\r\n",
                "\r\n",
                "# Inspect column names from the table\r\n",
                "result = spark.sql(\"SHOW COLUMNS FROM tablename\")\r\n",
                "\r\n",
                "# Second method to inspect table\r\n",
                "result = spark.sql(\"SELECT * FROM tablename LIMIT 0\")\r\n",
                "\r\n",
                "# anther way to inspect table\r\n",
                "result = spark.sql(\"DESCRIBE tablename\")\r\n",
                "\r\n",
                "# to see column names from the result\r\n",
                "result.show()\r\n",
                "\r\n",
                "# another way to find columns from the result\r\n",
                "print(result.columns)\r\n",
                "\r\n",
                "\r\n",
                "# Another example of creating the table and inspecting the columns\r\n",
                "# Load trainsched.txt\r\n",
                "df = spark.read.csv(\"trainsched.txt\", header=True)\r\n",
                "\r\n",
                "# Create temporary table called schedule\r\n",
                "df.createOrReplaceTempView(\"schedule\")\r\n",
                "\r\n",
                "# Inspect the columns in the table df\r\n",
                "spark.sql(\"DESCRIBE schedule\").show()\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "0134abe8-0ac2-4db2-a01d-12ee49e9fe08"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Window function Spark SQL\n",
                "\n",
                "- A Window function operates on a set of rows and returns a value for each row in the set – but now this value can depend on other rows in the set.\n",
                "-  The term window describes the set of rows on which the function operates. The value returned for each row can be a value from one of the rows in the “window”, or, a value from a “window function” that uses values from the rows in the window to calculate its value"
            ],
            "metadata": {
                "azdata_cell_guid": "30bf6033-579d-4627-afb3-84c36f09eff3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "query = \"\"\"SELECT train_id, station, time, LEAD(time, 1) OVER (ORDER BY time) AS time_next FROM sched WHERE train_id=324 \"\"\"\r\n",
                "spark.sql(query).show()\r\n",
                "\r\n",
                "# Remove the train limitation\r\n",
                "query = \"\"\"SELECT train_id, station, time, LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time) AS time_next FROM sched \"\"\"\r\n",
                "\r\n",
                "# Example 2\r\n",
                "# Add col running_total that sums diff_min col in each group\r\n",
                "query = \"\"\"\r\n",
                "SELECT train_id, station, time, diff_min,\r\n",
                "SUM(diff_min) OVER (PARTITION BY train_id ORDER BY time) AS running_total\r\n",
                "FROM schedule\r\n",
                "\"\"\"\r\n",
                "\r\n",
                "# Run the query and display the result\r\n",
                "spark.sql(query).show()\r\n",
                "\r\n",
                "# Example 3\r\n",
                "query = \"\"\"\r\n",
                "SELECT \r\n",
                "ROW_NUMBER() OVER (ORDER BY time) AS row,\r\n",
                "train_id, \r\n",
                "station, \r\n",
                "time, \r\n",
                "LEAD(time,1) OVER (ORDER BY time) AS time_next \r\n",
                "FROM schedule\r\n",
                "\"\"\"\r\n",
                "spark.sql(query).show()\r\n",
                "\r\n",
                "# Give the number of the bad row as an integer\r\n",
                "bad_row = 7\r\n",
                "\r\n",
                "# Provide the missing clause, SQL keywords in upper case\r\n",
                "clause = 'PARTITION BY train_id'\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "dec77fa5-02f3-48be-8990-36c152cdf0ed",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Dot notation and SQL\n",
                "\n",
                "- Most Spark sql queries can be done in either dot notation or sql notation\n",
                "- Querying dataframes using dataframe dot notation.\n",
                "    1. Select Columns\n",
                "    2. Rename Columns\n",
                "    3. Window Function\n",
                "- Examples\n",
                "   1. ROW_NUMBER in SQL : `pyspark.sql.functions.row_number`\n",
                "   2. The inside of the OVER clause : `pyspark.sql.Window`\n",
                "   3. PARTITIONBY : `pyspark.sql.Window.partitionBy `\n",
                "   4. ORDERBY : p`yspark.sql.Window.orderBy`\n",
                "\n",
                "- A \"WindowSpec\" is defined using the \"Window\" class, and then used subsequently as an argument to the over() function in a window function query.\n",
                "\n",
                "  - The over function in SparkSQL corresponds to a OVER clause in SQL.\n",
                "  - The class `pyspark.sql.window.Window `represents the inside of an OVER \n",
                "  - type(window) is `pyspark.sql.window.WindowSpec`\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "2f58a09d-afb2-4c9e-ad22-a810cf319da6"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# show all columns\r\n",
                "df.columns['train_id', 'station', 'time']\r\n",
                "\r\n",
                "# show first 5 rows\r\n",
                "df.show(5)\r\n",
                "\r\n",
                "# show only two columns\r\n",
                "df.select('train_id','station').show(5)\r\n",
                "\r\n",
                "# second method to do same thing\r\n",
                "df.select(df.train_id, df.station)\r\n",
                "\r\n",
                "# third method to do same thing\r\n",
                "from pyspark.sql.functions import col\r\n",
                "df.select(col('train_id'), col('station'))\r\n",
                "\r\n",
                "# Rename columns\r\n",
                "df.select('train_id','station').withColumnRenamed('train_id','train').show(5)\r\n",
                "\r\n",
                "# Another method to rename\r\n",
                "df.select(col('train_id').alias('train'), 'station')\r\n",
                "\r\n",
                "# Don't do this i.e. trying to use all the three ways/conventions of refering the columns at the same time without good reason.\r\n",
                "df.select('train_id', df.station, col('time'))\r\n",
                "\r\n",
                "# Sample Query using sql notation\r\n",
                "spark.sql('SELECT train_id AS train, station FROM schedule LIMIT 5').show()\r\n",
                "\r\n",
                "# same query using dot notation\r\n",
                "df.select(col('train_id').alias('train'), 'station')\r\n",
                "  .limit(5)\r\n",
                "  .show()\r\n",
                "\r\n",
                "query = \"\"\"\r\n",
                "SELECT *,\r\n",
                " ROW_NUMBER() OVER(PARTITION BY train_id ORDER BY time) AS id\r\n",
                "  FROM schedule\"\"\"\r\n",
                "spark.sql(query)\r\n",
                "     .show(11)\r\n",
                "\r\n",
                "# Dot notaion equivalent\r\n",
                "from pyspark.sql import Window,\r\n",
                "from pyspark.sql.functions import row_number\r\n",
                " df.withColumn(\"id\", row_number()\r\n",
                "                      .over(\r\n",
                "                               Window.partitionBy('train_id')\r\n",
                "                                     .orderBy('time')\r\n",
                "                           ) \r\n",
                "             )\r\n",
                "\r\n",
                "# WindowSpec example\r\n",
                "clause.window = Window.partitionBy('train_id').orderBy('time')\r\n",
                "dfx = df.withColumn('next', lead('time',1).over(window))"
            ],
            "metadata": {
                "azdata_cell_guid": "d2bb6840-91a6-4356-a758-5909adc2b42f"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Example 1\r\n",
                "# Give the identical result in each command\r\n",
                "spark.sql('SELECT train_id, MIN(time) AS start FROM schedule GROUP BY train_id').show()\r\n",
                "df.groupBy('train_id').agg({'time':'min'}).withColumnRenamed('min(time)', 'start').show()\r\n",
                "\r\n",
                "# Print the second column of the result\r\n",
                "spark.sql('SELECT train_id, MIN(time), MAX(time) FROM schedule GROUP BY train_id').show()\r\n",
                "result = df.groupBy('train_id').agg({'time':'min', 'time':'max'})\r\n",
                "result.show()\r\n",
                "print(result.columns[1])\r\n",
                "\r\n",
                "\r\n",
                "# Example 2\r\n",
                "from pyspark.sql.functions import min, max, col\r\n",
                "expr = [min(col(\"time\")).alias('start'), max(col(\"time\")).alias('end')]\r\n",
                "dot_df = df.groupBy(\"train_id\").agg(*expr)\r\n",
                "dot_df.show()\r\n",
                "\r\n",
                "# Write a SQL query giving a result identical to dot_df\r\n",
                "query = \"SELECT train_id, MIN(time) AS start, MAX(time) AS end FROM schedule GROUP BY train_id\"\r\n",
                "sql_df = spark.sql(query)\r\n",
                "sql_df.show()\r\n",
                "\r\n",
                "# Example 3\r\n",
                "df = spark.sql(\"\"\"\r\n",
                "SELECT *, \r\n",
                "LEAD(time,1) OVER(PARTITION BY train_id ORDER BY time) AS time_next \r\n",
                "FROM schedule\r\n",
                "\"\"\")\r\n",
                "\r\n",
                "# Obtain the identical result using dot notation \r\n",
                "dot_df = df.withColumn('time_next', lead('time', 1)\r\n",
                "        .over(Window.partitionBy('train_id')\r\n",
                "        .orderBy('time')))\r\n",
                "\r\n",
                "# Example 4\r\n",
                "window = Window.partitionBy('train_id').orderBy('time')\r\n",
                "dot_df = df.withColumn('diff_min', \r\n",
                "                    (unix_timestamp(lead('time', 1).over(window),'H:m') \r\n",
                "                     - unix_timestamp('time', 'H:m'))/60)\r\n",
                "\r\n",
                "\r\n",
                "# Create a SQL query to obtain an identical result to dot_df\r\n",
                "query = \"\"\"\r\n",
                "SELECT *, \r\n",
                "(UNIX_TIMESTAMP(LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time),'H:m') \r\n",
                " - UNIX_TIMESTAMP(time, 'H:m'))/60 AS diff_min \r\n",
                "FROM schedule \r\n",
                "\"\"\"\r\n",
                "sql_df = spark.sql(query)\r\n",
                "sql_df.show()"
            ],
            "metadata": {
                "azdata_cell_guid": "34110496-8e93-4038-9752-4cb1979c47c3"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Natural Language Processing using Spark SQL\r\n",
                "\r\n",
                "- Load Natural Language text into a dataframe while discarding all the unwanted data.\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "bdcce14b-c3ac-4970-8f10-d8335fcb4ccc"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Load text file\r\n",
                "df = spark.read.text('sherlock.txt')\r\n",
                "\r\n",
                "# Gets the first row\r\n",
                "print(df.first())\r\n",
                "\r\n",
                "# Counts the rows\r\n",
                "print(df.count())\r\n",
                "\r\n",
                "# Supports reading from multiple file formats\r\n",
                "df1 = spark.read.load('sherlock.parquet')\r\n",
                "\r\n",
                "# Prints first 15 lines\r\n",
                "# Truncate - FALSE allows to print long rows and does not truncate them\r\n",
                "df1.show(15, truncate=False)\r\n",
                "\r\n",
                "# Converts a column to lower case\r\n",
                "df = df1.select(lower(col('value')))\r\n",
                "print(df.first())\r\n",
                "\r\n",
                "# Result column name \"lower(value)\"\r\n",
                "df.columns\r\n",
                "\r\n",
                "# Alias operation allows us to give new name to a new column\r\n",
                "df = df1.select(lower(col('value')).alias('v'))\r\n",
                "\r\n",
                "# The operation regexp_replace replaces values that match a pattern.\r\n",
                "# The first argument is the column name. The second argument is the pattern to be replaced. It replaces every occurrence of the second argument with the third argument.\r\n",
                "# To prevent the period from being interpreted as a special character in the second argument, we put a backslash in front of it. This is called \"escaping\" it.\r\n",
                "# We must also escape other special characters such as a single quote.\r\n",
                "df = df1.select(regexp_replace('value', 'Mr\\.', 'Mr').alias('v'))\r\n",
                "df = df1.select(regexp_replace('value', 'don\\'t', 'do not').alias('v'))\r\n",
                "\r\n",
                "\r\n",
                "# The split operation separates a string into individual tokens.\r\n",
                "# The second argument gives the list of characters on which to split. Here it is a space.\r\n",
                "# Retuns an array of strings\r\n",
                "df = df2.select(split('v', '[ ]').alias('words'))\r\n",
                "\r\n",
                "# Handle punctuation marks..in addition to the space it discards unwanted symbols\r\n",
                "punctuation = \"_|.\\?\\!\\\",\\'\\[\\]\\*()\"\r\n",
                "df3 = df2.select(split('v', '[ %s]' % punctuation).alias('words'))\r\n",
                "\r\n",
                "# explode() takes an array of things, and puts each thing on its own row, preserving the order.\r\n",
                "df4 = df3.select(explode('words').alias('word'))\r\n",
                "print(df4.count())\r\n",
                "nonblank_df = df.where(length('word') > 0) # Remove blank rows\r\n",
                "print(nonblank_df.count())\r\n",
                "\r\n",
                "# The monotonically_increasing_id() operation efficiently creates a column of integers that are always increasing.\r\n",
                "# Here we are using it to create a column of unique IDs for each row.\r\n",
                "df2 = df.select('word', monotonically_increasing_id().alias('id'))\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "822119e7-c09b-4495-9464-100eec0f19f7"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Partitioning Data\r\n",
                "\r\n",
                "- Partitioning allows Spark to parallelize operations. We will organize the data allow window functions to use the partition clause. \r\n",
                "\r\n",
                "- The `when/otherwise` operation is a case statement. The first argument gives the condition. The second argument gives the desired value for the column. You can chain multiple when() operations. \r\n",
                "- The last `when()` operation is followed by an `otherwise()` clause that gives the column value used if none of the previous conditions applies. When combined with the `withColumn` operation when/otherwise groups the data into chapters. Repeating this adds a part id column."
            ],
            "metadata": {
                "azdata_cell_guid": "cfc90ea0-53e3-4057-ab75-63e7230cda54"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Adding title/chapter column\r\n",
                "df2 = df.withColumn('title', when(df.id < 25000, 'Preface')                             \r\n",
                "                             .when(df.id < 50000, 'Chapter 1')\r\n",
                "                             .when(df.id < 75000, 'Chapter 2')      \r\n",
                "                             .otherwise('Chapter 3'))\r\n",
                "                             \r\n",
                "# Adding aprtid column                             \r\n",
                "df2 = df2.withColumn('part', when(df2.id < 25000, 0)\r\n",
                "                            .when(df2.id < 50000, 1)\r\n",
                "                            .when(df2.id < 75000, 2)\r\n",
                "                            .otherwise(3))            \r\n",
                "                            .show()\r\n",
                "\r\n",
                "# Repartition a column\r\n",
                "# First argument gives the desried number of partitions\r\n",
                "# The second argument is saying put rows having the same part column value into the same partition.\r\n",
                "df2 = df.repartition(4, 'part')\r\n",
                "print(df2.rdd.getNumPartitions())\r\n",
                "\r\n",
                "# Reading Pre-partitioned text files\r\n",
                "# spark.read.text() tells Spark to load all of the text files in the folder into a dataframe.\r\n",
                "# If available parallelism is more than one and the folder contains more than one file, this reads the files in parallel \r\n",
                "# and distributes the files over multiple partitions.\r\n",
                "df_parts = spark.read.text('sherlock_parts')\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "4214abed-9264-4d9d-a40b-2e9dc56f6e5c"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Load the dataframe\r\n",
                "df = spark.read.load('sherlock_sentences.parquet')\r\n",
                "\r\n",
                "# Filter and show the first 5 rows\r\n",
                "df.where('id > 70').show(5, truncate=False)\r\n",
                "\r\n",
                "# Split the clause column into a column called words \r\n",
                "split_df = clauses_df.select(split('clause', ' ').alias('words'))\r\n",
                "split_df.show(5, truncate=False)\r\n",
                "\r\n",
                "# Explode the words column into a column called word \r\n",
                "exploded_df = split_df.select(explode('words').alias('word'))\r\n",
                "exploded_df.show(10)\r\n",
                "\r\n",
                "# Count the resulting number of rows in exploded_df\r\n",
                "print(\"\\nNumber of rows: \", exploded_df.count())"
            ],
            "metadata": {
                "azdata_cell_guid": "5ded769e-d2a9-4d54-9395-c71cad8760ab"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Moving Window Analysis\n",
                "\n",
                "- This dataset has already been processed to remove unwanted characters and put one word per row. An id column was added to identify the position of each word in the document.\n",
                "    \n",
                "- The data is partitioned into 12 parts, corresponding to chapters, each having a unique \"part\" and \"title\" column. The `distinct()` operation eliminates duplicates, fetching unique records. This will allow us to easily parallelize our work across up to 12 machines in a cluster, or up to 12 cores in a CPU on a single machine."
            ],
            "metadata": {
                "azdata_cell_guid": "2e5924a5-b17f-40d9-8bd8-0a080d525395"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "\r\n",
                "df.select('part', 'title').distinct().sort('part').show(truncate=False)\r\n",
                "\r\n",
                "# LEAD Window Function - Generates a sliding window/3-tuple\r\n",
                "query = \"\"\"\r\n",
                "SELECT id, word AS w1,\r\n",
                "LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\r\n",
                "LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3   \r\n",
                "FROM df\r\n",
                "  \"\"\"\r\n",
                "spark.sql(query).sort('id').show()\r\n",
                "\r\n",
                "# LAG Window Function\r\n",
                "lag_query = \"\"\" \r\n",
                "SELECT    id,\r\n",
                "LAG(word,2) OVER(PARTITION BY part ORDER BY id ) AS w1,\r\n",
                "LAG(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\r\n",
                "word AS w3\r\n",
                "FROM df\r\n",
                "ORDER BY id\"\"\"\r\n",
                "spark.sql(lag_query).show()\r\n",
                "\r\n",
                "# Word for each row, previous two and subsequent two words\r\n",
                "query = \"\"\"\r\n",
                "SELECT\r\n",
                "part,\r\n",
                "LAG(word, 2) OVER(PARTITION BY part ORDER BY id) AS w1,\r\n",
                "LAG(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\r\n",
                "word AS w3,\r\n",
                "LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) AS w4,\r\n",
                "LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) AS w5\r\n",
                "FROM text\r\n",
                "\"\"\"\r\n",
                "spark.sql(query).where(\"part = 12\").show(10)\r\n",
                "\r\n",
                "# Repartition text_df into 12 partitions on 'chapter' column\r\n",
                "repart_df = text_df.repartition(12,'chapter')\r\n",
                "\r\n",
                "# Prove that repart_df has 12 partitions\r\n",
                "repart_df.rdd.getNumPartitions()\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "31b22f85-012f-4f0c-9084-fbe0d69dc9a5"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Common Word Sequences\r\n",
                "\r\n",
                "- Most frequent word sequences in a natural language text document.\r\n",
                "- Application is in Sequence Prediction/Endword Prediction\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "4d5d5797-4c42-46e0-9b06-9c9815255c92"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "query3agg = \"\"\"\r\n",
                "SELECT w1,\r\n",
                "w2,\r\n",
                "w3,\r\n",
                "COUNT(*) as count FROM (\r\n",
                "    SELECT    word AS w1,\r\n",
                "    LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\r\n",
                "    LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3 FROM df\r\n",
                "    )\r\n",
                "GROUP BY w1, w2, w3\r\n",
                "ORDER BY count DESC\"\"\"\r\n",
                "\r\n",
                "spark.sql(query3agg).show()\r\n",
                "\r\n",
                "query3agg = \"\"\"\r\n",
                "SELECT w1,\r\n",
                "w2,\r\n",
                "w3,\r\n",
                "length(w1)+length(w2)+length(w3) as length\r\n",
                "FROM (\r\n",
                "    SELECT    word AS w1,\r\n",
                "    LEAD(word,1) OVER(PARTITION BY part ORDER BY id ) AS w2,\r\n",
                "    LEAD(word,2) OVER(PARTITION BY part ORDER BY id ) AS w3 \r\n",
                "    FROM df\r\n",
                "    WHERE part <> 0 and part <> 13\r\n",
                "    )\r\n",
                "GROUP BY w1, w2, w3\r\n",
                "ORDER BY length DESC\r\n",
                "\"\"\"\r\n",
                "spark.sql(query3agg).show(truncate=False)\r\n",
                "\r\n",
                "# Find the top 10 sequences of five words\r\n",
                "query = \"\"\"\r\n",
                "SELECT w1, w2, w3, w4, w5, COUNT(*) AS count FROM (\r\n",
                "   SELECT word AS w1,\r\n",
                "   LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\r\n",
                "   LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) AS w3,\r\n",
                "   LEAD(word, 3) OVER(PARTITION BY part ORDER BY id) AS w4,\r\n",
                "   LEAD(word, 4) OVER(PARTITION BY part ORDER BY id) AS w5\r\n",
                "   FROM text\r\n",
                ")\r\n",
                "GROUP BY w1, w2, w3, w4, w5\r\n",
                "ORDER BY count DESC\r\n",
                "LIMIT 10 \"\"\"\r\n",
                "df = spark.sql(query)\r\n",
                "df.show()\r\n",
                "\r\n",
                "# Unique 5-tuples sorted in descending order\r\n",
                "query = \"\"\"\r\n",
                "SELECT Distinct w1, w2, w3, w4, w5 FROM (\r\n",
                "   SELECT word AS w1,\r\n",
                "   LEAD(word, 1) OVER(PARTITION BY part ORDER BY id) AS w2,\r\n",
                "   LEAD(word, 2) OVER(PARTITION BY part ORDER BY id) AS w3,\r\n",
                "   LEAD(word, 3) OVER(PARTITION BY part ORDER BY id) AS w4,\r\n",
                "   LEAD(word, 4) OVER(PARTITION BY part ORDER BY id) AS w5\r\n",
                "   FROM text\r\n",
                ")\r\n",
                "ORDER BY w1 DESC, w2 DESC, w3 DESC, w4 DESC, w5 DESC \r\n",
                "LIMIT 10\r\n",
                "\"\"\"\r\n",
                "df = spark.sql(query)\r\n",
                "df.show()\r\n",
                "\r\n",
                "# Using Subquery\r\n",
                "subquery = \"\"\"\r\n",
                "SELECT chapter, w1, w2, w3, COUNT(*) as count\r\n",
                "FROM\r\n",
                "(\r\n",
                "    SELECT\r\n",
                "    chapter,\r\n",
                "    word AS w1,\r\n",
                "    LEAD(word, 1) OVER(PARTITION BY chapter ORDER BY id ) AS w2,\r\n",
                "    LEAD(word, 2) OVER(PARTITION BY chapter ORDER BY id ) AS w3\r\n",
                "    FROM text\r\n",
                ")\r\n",
                "GROUP BY chapter, w1, w2, w3\r\n",
                "ORDER BY chapter, count DESC\r\n",
                "\"\"\"\r\n",
                "\r\n",
                "#   Most frequent 3-tuple per chapter\r\n",
                "query = \"\"\"\r\n",
                "SELECT chapter, w1, w2, w3, count FROM\r\n",
                "(\r\n",
                "  SELECT\r\n",
                "  chapter,\r\n",
                "  ROW_NUMBER() OVER (PARTITION BY chapter ORDER BY count DESC) AS row,\r\n",
                "  w1, w2, w3, count\r\n",
                "  FROM ( %s )\r\n",
                ")\r\n",
                "WHERE row = 1\r\n",
                "ORDER BY chapter ASC\r\n",
                "\"\"\" % subquery\r\n",
                "\r\n",
                "spark.sql(query).show()"
            ],
            "metadata": {
                "azdata_cell_guid": "d4a9a50c-0dc0-42db-9d9a-0ac38ff03802"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Caching\n",
                "\n",
                "What is caching?\n",
                "\n",
                "- Keeping data in memory so that it does not have to be refetched or recalculated each time it is used.\n",
                "- Spark tends to unload memory aggressively. It will err on the side of unloading data from memory if it is not used, even if it is going to be needed later\n",
                "- Caching is a lazy operation. A dataframe won't appear in the cache until an action is performed on the dataframe. Don't overdo it.\n",
                "-  Only cache if more than one operation is to be performed and it takes substantial time to create the dataframe. Unpersist unneeded objects. Caching incurs a cost. Caching everything generally slows things down.\n",
                "\n",
                "\n",
                "\n",
                "## Eviction Policy\n",
                "\n",
                "- Eviction Policy determines when and which data is removed from cache. The policy is LRU. Each worker manages its own cache, and eviction depends on the memory available to each worker.\n",
                "- LeastRecentlyUsed(LRU) policy\n",
                "- Eviction happens independently on each worker\n",
                "- Depends on memory available to each worker\n",
                "\n",
                "## Storage Level\n",
                "In the `storagelevel` above the following hold\n",
                "\n",
                "- useDisk = True (specifies whether to move some or all of the dataframe to disk if it needed to free up memory)\n",
                "- useMemory = True (specifies whether to keep the data in memory)\n",
                "- useOffHeap = False \n",
                "   - tells Spark to use off-heap storage instead of on-heap memory. \n",
                "   - The on-heap store refers to objects in an in-memory data structure that is fast to access. \n",
                "   - The off-heap store is also in memory, but is slightly slower than the on-heap store. However, off-heap storage is still faster than disk. \n",
                "   - Even though the best performance is obtained when operating solely in on-heap memory, Spark also makes it possible to use off-heap storage for certain operations. \n",
                "   - Off-heap storage is slightly slower than on-heap but still faster than disk. The downside is that the user has to manually deal with managing the allocated memory\n",
                "- deserialized = True (deserialized True is faster but uses more memory. Serialized data is more space-efficient but slower to read. This option only applies to in-memory storage. Disk cache is always serialized.)\n",
                "- replication = 1 (replication is used to tell Spark to replicate data on multiple nodes. This allows faster fault recovery when a node fails.)\n",
                "\n",
                "## Persist\n",
                "\n",
                "- The `persist()` command allows you to specify the desired storage level using the first argument.\n",
                "-  If that argument is not provided, it uses a default setting. When memory is scarce, it is recommended to use MEMORY_AND_DISK caching strategy. \n",
                "- This will spill the dataframe to disk if memory runs low. Reading the dataframe from disk cache is slower than reading it from memory, but can still be faster than recreating from scratch. `cache()` is equivalent to using `persist()` with the default storageLevel."
            ],
            "metadata": {
                "azdata_cell_guid": "0a49ccc0-9fcb-4a0a-9673-97d38076cd80"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "#To cache dataframe\r\n",
                "df.cache()\r\n",
                "\r\n",
                "#To uncache dataframe\r\n",
                "df.unpersist()\r\n",
                "\r\n",
                "# Confirm whether dataframe is cached or uncached\r\n",
                "df.is_cached\r\n",
                "\r\n",
                "# Five Details about how dataframe is cached\r\n",
                "df.storageLevel\r\n",
                "\r\n",
                "# df.cache() is shorthand for df.persist() with the first argument set to its default value\r\n",
                "df.persist()\r\n",
                "\r\n",
                "# persist with storagelevel set to pyspark.StorageLevel.MEMORY_AND_DISK\r\n",
                "df.persist(storageLevel=pyspark.StorageLevel.MEMORY_AND_DISK)\r\n",
                "\r\n",
                "# List the tables\r\n",
                "print(\"Tables:\\n\", spark.catalog.listTables())\r\n",
                "\r\n",
                "# cache table and check if it is cached \r\n",
                "df.createOrReplaceTempView('df')\r\n",
                "spark.catalog.cacheTable('df')\r\n",
                "spark.catalog.isCached(tableName='df')\r\n",
                "\r\n",
                "# uncache a table\r\n",
                "spark.catalog.uncacheTable('df')\r\n",
                "\r\n",
                "# removes all cached tables.\r\n",
                "spark.catalog.clearCache()\r\n",
                "\r\n",
                "# removes a table from the cache\r\n",
                "spark.catalog.dropTempView('table1')\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "4fdc1a2d-1492-4582-904a-53fbd7bc14cc"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Spark UI\n",
                "\n",
                "- Spark UI is a web interface to inspect Spark execution\n",
                "- `Spark Task` is a unit of execution that runs on a single cpu\n",
                "- `Spark Stage` a group of tasks that perform the same computation in parallel,each task typically running on a different subset of the data\n",
                "- `Spark Job` is a computation triggered by an action, sliced into one or more stages."
            ],
            "metadata": {
                "azdata_cell_guid": "0a585025-2d03-4f4b-8488-9dec48f161a0"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Logging\n",
                "\n",
                "- Logging and how to avoid stealth loss of cpu when logging spark actions\n",
                "- If you are deploying Spark jobs to production then logging is important, In Pyspark this is especially very important if you are using log statements to inspect dataframes.\n",
                "- The `level` argument of the `basicConfig()` function sets the logging level to info level, and has each log message print the time, the log level of the statement, and the message itself.\n",
                "- In the below example the first statement printed, whereas the second one did not. This is because the second log statement is at debug level.\n",
                "- Due to the combination of Spark's lazy evaluation and distributed computation, debugging a complex application can be challenging.\n",
                "- This is because an erroneous action might not be triggered until well downstream of where the action is coded. One way to simplify the debugging of an application is to force actions to be triggered. However, a naive approach to doing this can cause stealth loss of CPU.\n",
                "- Spark operations that trigger an action must be logged with care to avoid stealth loss of compute resource.Allow for dataframe actions that you need during development or during debugging, and  disable them in production, while doing so with confidence that they will not be silently executed."
            ],
            "metadata": {
                "azdata_cell_guid": "9e3aa7d4-3aad-474b-b40d-aa1828186e4d"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Logging with INFO Level\r\n",
                "import logging\r\n",
                "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\r\n",
                "                    format='%(asctime)s - %(levelname)s - %(message)s')\r\n",
                "logging.info(\"Hello %s\", \"world\")\r\n",
                "logging.debug(\"Hello, take %d\", 2)\r\n",
                "\r\n",
                "# Logging with DEBUG Level\r\n",
                "import logging\r\n",
                "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, \r\n",
                "                                  format='%(asctime)s - %(levelname)s - %(message)s')\r\n",
                "logging.info(\"Hello %s\", \"world\")\r\n",
                "logging.debug(\"Hello, take %d\", 2)\r\n",
                "\r\n",
                "# Even though logging Level is INFO the debug statement will be executed and not printed becaause Spark actions are executed\r\n",
                "import logging\r\n",
                "logging.basicConfig(level=logging.INFO,\r\n",
                "                     format='%(asctime)s - %(levelname)s - %(message)s')\r\n",
                "# < create dataframe df here >\r\n",
                "t = timer()\r\n",
                "logging.info(\"No action here.\")\r\n",
                "t.elapsed()\r\n",
                "logging.debug(\"df has %d rows.\", df.count())\r\n",
                "t.elapsed()\r\n",
                "\r\n",
                "# Disable Action to avoid Stealth Loss of CPU\r\n",
                "ENABLED = False\r\n",
                "t = timer()\r\n",
                "logger.info(\"No action here.\")\r\n",
                "t.elapsed()\r\n",
                "if ENABLED:\r\n",
                "       logger.info(\"df has %d rows.\", df.count())\r\n",
                "t.elapsed()\r\n",
                "\r\n",
                "\r\n",
                "# Log columns of text_df as debug message\r\n",
                "logging.debug(\"text_df columns: %s\", text_df.columns)\r\n",
                "\r\n",
                "# Log whether table1 is cached as info message\r\n",
                "logging.info(\"table1 is cached: %s\", spark.catalog.isCached(tableName=\"table1\"))\r\n",
                "\r\n",
                "# Log first row of text_df as warning message\r\n",
                "logging.warning(\"The first row of text_df:\\n %s\", text_df.first())\r\n",
                "\r\n",
                "# Log selected columns of text_df as error message\r\n",
                "logging.error(\"Selected columns: %s\", text_df.select(\"id\", \"word\"))\r\n",
                "\r\n",
                "# Uncomment the 5 statements that do NOT trigger text_df\r\n",
                "logging.debug(\"text_df columns: %s\", text_df.columns)\r\n",
                "logging.info(\"table1 is cached: %s\", spark.catalog.isCached(tableName=\"table1\"))\r\n",
                "# logging.warning(\"The first row of text_df: %s\", text_df.first())\r\n",
                "logging.error(\"Selected columns: %s\", text_df.select(\"id\", \"word\"))\r\n",
                "logging.info(\"Tables: %s\", spark.sql(\"show tables\").collect())\r\n",
                "logging.debug(\"First row: %s\", spark.sql(\"SELECT * FROM table1 limit 1\"))\r\n",
                "#logging.debug(\"Count: %s\", spark.sql(\"SELECT COUNT(*) AS count FROM table1\").collect())"
            ],
            "metadata": {
                "azdata_cell_guid": "102cff5c-1e7c-4588-9631-58254378fcde"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Query Plans\n",
                "\n",
                "If you put the `Explain` keyword at the head of an sql query, running the query provides detailed plan information about the query without actually running it. Instead of the usual table of data, it returns a query execution plan, also called a query plan. A query plan is a string, representing a set of steps used to access the data.\n",
                "\n",
                "- It tells us that it read the data from a parquet file, having 4 columns along with their names, located at /temp/df.parquet. It also tells us the schema of the table, including the column types. This allows us to determine how the data was obtained and from where.\n",
                "- Query plan start from the bottom first."
            ],
            "metadata": {
                "azdata_cell_guid": "30ca6fbe-db5b-48ce-8848-f38a96f1f01d"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "df = sqlContext.read.load('/temp/df.parquet')\r\n",
                "df.registerTempTable('df')\r\n",
                "\r\n",
                "# explain on query result\r\n",
                "spark.sql('EXPLAIN SELECT * FROM df').first()\r\n",
                "\r\n",
                "# Expain on dataframe\r\n",
                "df.explain()\r\n",
                "\r\n",
                "# cache the dataframe and then explain\r\n",
                "df.cache()\r\n",
                "df.explain()"
            ],
            "metadata": {
                "azdata_cell_guid": "c675838c-7b65-4d9a-80f0-001c9b7b5276"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}