{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# BigData fundamentals with PySpark\r\n",
                "\r\n",
                "    - Apache Spark is originally written in Scala programming language. To support Python with Spark, PySpark was developed. Unlike previous versions, the newest version of PySpark provides computation power similar to Scala.\r\n",
                "\t\r\n",
                "\t- Spark comes with interactive shells that enable ad-hoc data analysis. Spark shell is an interactive environment through which one can access Spark's functionality quickly and conveniently. \r\n",
                "\t\r\n",
                "\t- Spark shell is particularly helpful for fast interactive prototyping before running the jobs on clusters. Unlike most other shells, Spark shell allow you to interact with data that is distributed on disk or in memory across many machines, and Spark takes care of automatically distributing this processing. Spark provides the shell in three programming languages: spark-shell for Scala, PySpark for Python and sparkR for R\r\n",
                "\t\r\n",
                "\t- Pyspark shell is the Python-based command line tool to develop Spark's interactive applications in Python. PySpark helps data scientists interface with Spark data structures in Apache Spark and python. Similar to Scala Shell, Pyspark shell has been augmented to support connecting to a cluster\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "f5af9f3d-8cbb-403a-bd5a-5a38597fe902"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Understanding SparkContext\r\n",
                "\r\n",
                "A SparkContext represents the entry point to Spark functionality. It's like a key to your car. PySpark automatically creates a SparkContext for you in the PySpark shell (so you don't have to create it by yourself) and is exposed via a variable sc.\r\n",
                "\r\n",
                "1. Print the version of SparkContext in the PySpark shell.\r\n",
                "2. Print the Python version of SparkContext in the PySpark shell.\r\n",
                "3. What is the master of SparkContext in the PySpark shell?"
            ],
            "metadata": {
                "azdata_cell_guid": "27765814-01aa-44da-8960-3d7634cbdd11"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Print the version of SparkContext\r\n",
                "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\r\n",
                "\r\n",
                "# Print the Python version of SparkContext\r\n",
                "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\r\n",
                "\r\n",
                "# Print the master of SparkContext\r\n",
                "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
            ],
            "metadata": {
                "azdata_cell_guid": "19362701-2eec-42cd-a7db-4b593384b70e"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Interactive Use of PySpark\r\n",
                "\r\n",
                "Spark comes with an interactive python shell in which PySpark is already installed in it. PySpark shell is useful for basic testing and debugging and it is quite powerful. The easiest way to demonstrate the power of PySpark’s shell is to start using it. In this example, you'll load a simple list containing numbers ranging from 1 to 100 in the PySpark shell.\r\n",
                "\r\n",
                "The most important thing to understand here is that we are not creating any SparkContext object because PySpark automatically creates the SparkContext object named sc, by default in the PySpark shell.\r\n",
                "\r\n",
                "1. Create a python list named numb containing the numbers 1 to 100.\r\n",
                "2. Load the list into Spark using Spark Context's parallelize method and assign it to a variable spark_data."
            ],
            "metadata": {
                "azdata_cell_guid": "95219a70-e837-4def-99a3-40c78f4ad743"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Create a python list of numbers from 1 to 100 \r\n",
                "numb = range(1, 100)\r\n",
                "\r\n",
                "# Load the list into PySpark  \r\n",
                "spark_data = sc.parallelize(numb)"
            ],
            "metadata": {
                "azdata_cell_guid": "380b3e39-8747-461c-ae51-8a26d1b89012",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Load File\r\n",
                "\r\n",
                "In PySpark, we express our computation through operations on distributed collections that are automatically parallelized across the cluster. In the previous example, you have seen a scenario of loading a list as parallelized collections and in this example, you'll load the data from a local file in PySpark shell.\r\n",
                "\r\n",
                "Remember you already have a SparkContext sc and file_path variable (which is the path to the README.md file) already available in your workspace.\r\n",
                "\r\n",
                "1. Load a local text file README.md in PySpark shell."
            ],
            "metadata": {
                "azdata_cell_guid": "d4184be1-0a33-4702-b5c1-5fd7ef603259"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Load a local file into PySpark shell\r\n",
                "lines = sc.textFile(file_path)"
            ],
            "metadata": {
                "azdata_cell_guid": "db518a98-1bee-4edc-bb37-5897dcd7d4d8",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Review of functional programming in Python\r\n",
                "\r\n",
                "Understanding python becomes easier if we understand functional programming principles in python. \r\n",
                "\r\n",
                "Anonymous Functions in python are functions that are not bound to a name at runtime, using a construct called a `lambda`. Used in conjunction with `map` and `filter` functions. Similar to `def` lambda creates a function to be later called in the program. However, it returns a function instead of assigning it to a name. This is the reason lambdas are called as anonymous functions.\r\n",
                "\r\n",
                "In practice, they are used as a way to inline a function definition, or to defer execution of a code. \r\n",
                "\r\n",
                "Lambda functions can be used whenever function objects are required. They can have any number of arguments but only one expression and the expression is evaluated and returned. \r\n",
                "\r\n",
                "Lambda function syntax: `lambda arguments: expression`\r\n",
                "\r\n",
                "The main difference between `def` and `lambda` is that `lambda` defintion does not include a return statement and it always contains an expression that could be returned.\r\n",
                "Also note we can put a lambda definition anywhere a function is expected, and we don't have to assign it to a variable at all\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "We use `lambda` functions when we need a nameless function for a short period of time.\r\n",
                "`map()` function takes a function and a list and returns a new list which contains items returned by the function for each item.\r\n",
                "\r\n",
                "General Syntax: `map(function, list)`\r\n",
                "\r\n",
                "`filter()` function takes a function and a list and returns a new list for which the function evaluates as true.\r\n",
                "\r\n",
                "General Syntax: `filter(function, list)`\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "34ebfc2a-66c4-4d66-b41a-6e354ea4e04b"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "double = lambda x: x*2\r\n",
                "print(double(3))"
            ],
            "metadata": {
                "azdata_cell_guid": "7132265a-8373-45cc-9ce9-858eed6b9876"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Example of map() with lambda\r\n",
                "items = [1, 2, 3, 4]\r\n",
                "list(map(lambda x: x + 2, items))\r\n",
                "\r\n",
                "# Example of filter with lambda\r\n",
                "list(filter(lambda x: (x%2 !=0), items))"
            ],
            "metadata": {
                "azdata_cell_guid": "5cb51dbf-8d48-45ef-893d-43c8b6207bc5"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 2,
                    "data": {
                        "text/plain": "[1, 3]"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Use of lambda() with map()\r\n",
                "\r\n",
                "The `map()` function in Python returns a list of the results after applying the given function to each item of a given iterable (list, tuple etc.). \r\n",
                "\r\n",
                "The general syntax of map() function is map(fun, iter). We can also use lambda functions with map(). The general syntax of map() function with lambda() is `map(lambda <agument>:<expression>, iter)`.\r\n",
                "\r\n",
                "The general syntax of the filter() function with lambda() is `filter(lambda <argument>:<expression>, list)`"
            ],
            "metadata": {
                "azdata_cell_guid": "fb3e77dd-5fe2-4c77-985a-1fb488f06e23"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n",
                "\r\n",
                "# Print my_list in the console\r\n",
                "print(\"Input list is\", my_list)\r\n",
                "\r\n",
                "# Square all numbers in my_list\r\n",
                "squared_list_lambda = list(map(lambda x: x ** 2, my_list))\r\n",
                "\r\n",
                "# Print the result of the map function\r\n",
                "print(\"The squared numbers are\", squared_list_lambda)\r\n",
                "\r\n",
                "my_list2 = [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\r\n",
                "\r\n",
                "# Print my_list2 in the console\r\n",
                "print(\"Input list is:\", my_list2)\r\n",
                "\r\n",
                "# Filter numbers divisible by 10\r\n",
                "filtered_list = list(filter(lambda x: (x%10 == 0), my_list2))\r\n",
                "\r\n",
                "# Print the numbers divisible by 10\r\n",
                "print(\"Numbers divisible by 10 are:\", filtered_list)"
            ],
            "metadata": {
                "azdata_cell_guid": "c0a3bfad-1350-4a9f-9b93-7a2a1f026dab",
                "tags": []
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Input list is [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nThe squared numbers are [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\nInput list is: [10, 21, 31, 40, 51, 60, 72, 80, 93, 101]\nNumbers divisible by 10 are: [10, 40, 60, 80]\n"
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "markdown",
            "source": [
                "# What is RDD\r\n",
                "\r\n",
                "`RDD ` = Resilient Distributed Datasets. It is simply a collection of data distributed across the cluster. Fundamental and backbone data type in Spark\r\n",
                "\r\n",
                "When Spark starts processing data, it divides the data into partitions and distributes the data across cluster nodes, with each node containing a slice of data\r\n",
                "\r\n",
                "![RDD](./Images/RDD1.png)\r\n",
                "\r\n",
                "\r\n",
                "`Resilient` - Ability to withstand failures (and recompute missing or damaged partitions)\r\n",
                "\r\n",
                "`Distributed` - Spanning across multiple machines(spanning the job across multiple machines for efficient computations)\r\n",
                "\r\n",
                "`Dataset` - Collection of partitioned data(ex. arrays, tables, tuples etc)\r\n",
                "\r\n",
                "## Creating RDD\r\n",
                "\r\n",
                "1. Parallelizing(`sc.parallelize()`) existing collection of datasets(ex. list or array or set)\r\n",
                "\r\n",
                "2. A more common way to create RDDs is to load data from external datasets such as ifles stored in HDFS or objects in Amazon S3 buckets, or from lines in atext file stored locally and pass it to SparContext's `textFile` method.\r\n",
                "\r\n",
                "3. Also from existing RDD's\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "e33cea8b-bed8-42a1-ac94-b08e7b4db4f7"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "numRDD = sc.parallelize([1, 2, 3, 4])\r\n",
                "helloRDD = sc.parallelize(\"Hello World\")\r\n",
                "fileRDD = sc.textFile(\"README.md\")\r\n",
                "\r\n",
                "# Confirm the type of the object\r\n",
                "type(helloRDD)"
            ],
            "metadata": {
                "azdata_cell_guid": "ebb69f56-dd83-462d-903c-f27740eee1ae"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Partitioning in PySpark\r\n",
                "\r\n",
                "A partition is a logical division of a large distributed data set. With each part being stored in multiple locations across the cluster.\r\n",
                "\r\n",
                "By default Spark partitions the data at the time of creating RDD based on several factors such as available resources, external datasets etc. However this behavior can be controlled by passing a second argument called as `minPartitions` which define the minimum no fo partitions to be created for an RDD.\r\n",
                "\r\n",
                "The number of partitons in an RDD can be found by using `getNumPartitions()` method\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "51c2f15e-d2df-4048-8076-6fb3725b7180"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "numRDD = sc.parallelize(range(10), minPartitions = 6)\r\n",
                "fileRDD = sc.parallelize(\"README.md\", minPartitions = 6)"
            ],
            "metadata": {
                "azdata_cell_guid": "d05cb071-d6c7-457f-af07-4ff047e6f76b"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Resilient Distributed Dataset (RDD) is the basic abstraction in Spark. It is an immutable distributed collection of objects. Since RDD is a fundamental and backbone data type in Spark, it is important that you understand how to create it (Example 1). \r\n",
                "\r\n",
                "PySpark can easily create RDDs from files that are stored in external storage devices such as HDFS (Hadoop Distributed File System), Amazon S3 buckets, etc. However, the most common method of creating RDD's is from files stored in your local file system. This method takes a file path and reads it as a collection of lines (Example 2).\r\n",
                "\r\n",
                "SparkContext's textFile() method takes an optional second argument called minPartitions for specifying the minimum number of partitions"
            ],
            "metadata": {
                "azdata_cell_guid": "88ce0536-9e40-42f1-9c8a-66ac157384bb"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Example 1\r\n",
                "# Create an RDD from a list of words\r\n",
                "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\r\n",
                "\r\n",
                "# Print out the type of the created object\r\n",
                "print(\"The type of RDD is\", type(RDD))\r\n",
                "\r\n",
                "# Example 2\r\n",
                "# Print the file_path\r\n",
                "print(\"The file_path is\", file_path)\r\n",
                "\r\n",
                "# Create a fileRDD from file_path\r\n",
                "fileRDD = sc.textFile(file_path)\r\n",
                "\r\n",
                "# Check the type of fileRDD\r\n",
                "print(\"The file type of fileRDD is\", type(fileRDD))\r\n",
                "\r\n",
                "# Example 3\r\n",
                "# Check the number of partitions in fileRDD\r\n",
                "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\r\n",
                "\r\n",
                "# Create a fileRDD_part from file_path with 5 partitions\r\n",
                "fileRDD_part = sc.textFile(file_path, minPartitions = 5)\r\n",
                "\r\n",
                "# Check the number of partitions in fileRDD_part\r\n",
                "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"
            ],
            "metadata": {
                "azdata_cell_guid": "79e39257-74e6-43c9-bc5e-5e91e05cce36"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Operations supported by RDDs\r\n",
                "\r\n",
                "1. Transformations: Operations that return an RDD\r\n",
                "\r\n",
                "2. Actions: Operations that perform computations on RDD\r\n",
                "\r\n",
                "The most important feature that helps RDD in fault tolerance and optimizing resource is **Lazy Evaluation**.\r\n",
                "\r\n",
                "## What is Lazy Evaluation\r\n",
                "\r\n",
                "Spark creates a graph from all the operations you perform on an RDD and execution of graph only starts when an action is performed on an RDD\r\n",
                "\r\n",
                "## Basic RDD Transformations\r\n",
                "\r\n",
                "`map()`, `filter()`, `flatMap()` and `union()`\r\n",
                "\r\n",
                "**map** transformation takes a function and applies to all the elements in the RDD.\r\n",
                "\r\n",
                "The main method by which you can manipulate data in PySpark is using map(). The map() transformation takes in a function and applies it to each element in the RDD. It can be used to do any number of things, from fetching the website associated with each URL in our collection to just squaring the numbers\r\n",
                "\r\n",
                "**filter** transformation takes in a condition and returns only the elements that pass the condition.\r\n",
                "\r\n",
                "**flatMap** returns multiple values for each element in the source RDD.(ex. splitting an input string into words)\r\n",
                "\r\n",
                "**union** transformation returns the union of one RDD with another RDD.\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "ee593f29-4bbf-4912-887c-65f2d338e80e"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# map example\r\n",
                "RDD = sc.parallelize([1,2,3,4])\r\n",
                "RDD_map = RDD.map(lambda x: x * x)\r\n",
                "\r\n",
                "# filter example\r\n",
                "RDD = sc.parallelize([1,2,3,4])\r\n",
                "RDD_filter = RDD.filter(lambda x: x > 2)\r\n",
                "\r\n",
                "# flatMap example\r\n",
                "RDD = sc.parallelize([\"Hello World\",\"How are you\"])\r\n",
                "RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \"))\r\n",
                "\r\n",
                "# union example\r\n",
                "inputRDD = sc.textFile(\"logs.txt\")\r\n",
                "errorRDD = inputRDD.filter(lambda x: \"error\"in x.split())\r\n",
                "warningsRDD = inputRDD.filter(lambda x: \"warnings\"in x.split())\r\n",
                "combinedRDD = errorRDD.union(warningsRDD)"
            ],
            "metadata": {
                "azdata_cell_guid": "f4464e68-9351-439c-be60-32f5360a1c24"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "# RDD Actions\r\n",
                "\r\n",
                "Operation return a value after running a computation on the RDD.\r\n",
                "\r\n",
                "## Four Basic Actions\r\n",
                "\r\n",
                "`collect`, `take(N)`, `first` and `count`.\r\n",
                "\r\n",
                "**collect** action returns complete list of elements from the RDD.\r\n",
                "\r\n",
                "**take(N)** retunrs an array with first N number of elements from the RDD\r\n",
                "\r\n",
                "**first** returns the firts element of an RDD\r\n",
                "\r\n",
                "**count** returns the number of elements in an RDD\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "12e39a04-1a5c-437a-9eab-737570f97148"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# collect example\r\n",
                "RDD_map.collect()\r\n",
                "\r\n",
                "# take example\r\n",
                "RDD_map.take(2)\r\n",
                "\r\n",
                "# first example\r\n",
                "RDD_map.first()\r\n",
                "\r\n",
                "#count example\r\n",
                "RDD_flatmap.count()"
            ],
            "metadata": {
                "azdata_cell_guid": "63b4c148-261d-401e-a0d6-c50a978f2134"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "1. Create map() transformation that cubes all of the numbers in numbRDD.\r\n",
                "2. Collect the results in a numbers_all variable.\r\n",
                "3. Print the output from numbers_all variable."
            ],
            "metadata": {
                "azdata_cell_guid": "5157bdd2-01de-412d-9c78-2912dd8c16a4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Create map() transformation to cube numbers\r\n",
                "cubedRDD = numbRDD.map(lambda x: x*x*x)\r\n",
                "\r\n",
                "# Collect the results\r\n",
                "numbers_all = cubedRDD.collect()\r\n",
                "\r\n",
                "# Print the numbers from numbers_all\r\n",
                "for numb in numbers_all:\r\n",
                "\tprint(numb)"
            ],
            "metadata": {
                "azdata_cell_guid": "49af522a-87dd-4820-a8ab-97b929c51084"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Filter and Count\r\n",
                "\r\n",
                "1. Create filter() transformation to select the lines containing the keyword Spark.\r\n",
                "\r\n",
                "2. How many lines in fileRDD_filter contains the keyword Spark?\r\n",
                "\r\n",
                "3. Print the first four lines of the resulting RDD."
            ],
            "metadata": {
                "azdata_cell_guid": "2f9bcf4e-7f3a-461f-90e7-16d5689a64d9"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Filter the fileRDD to select lines with Spark keyword\r\n",
                "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)\r\n",
                "\r\n",
                "# How many lines are there in fileRDD?\r\n",
                "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\r\n",
                "\r\n",
                "# Print the first four lines of fileRDD\r\n",
                "for line in fileRDD_filter.take(4): \r\n",
                "  print(line)"
            ],
            "metadata": {
                "azdata_cell_guid": "08593c86-2c59-4914-8ea4-c344edcc311a",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Pair RDDs in PySpark\r\n",
                "\r\n",
                "- Real life datasets are usually key/value pairs(common data type required for many operations in spark). Each row is a key and maps to one or more values. Pair RDD is a special data structure to work with this kind of datasets\r\n",
                "\r\n",
                "- Pair RDD: Key is the identier and value is data\r\n",
                "\r\n",
                "- Two common ways to create pair RDDs\r\n",
                "\r\n",
                "    1. From a list of key-value tuple\r\n",
                "\r\n",
                "    2. From a regular RDD\r\n",
                "    \r\n",
                "- **_*Note: Get the data into key/value form for paired RDD*_**\r\n",
                "\r\n",
                "- All regular transformations work on pair RDD. Need to pass functions that operate on key value pairs rather than on individual elements\r\n",
                "\r\n",
                "### - Examples of paired RDD Transformations\r\n",
                "\r\n",
                "1. **reduceByKey(func)**: Combine values with the same key using a function\r\n",
                " - It runs several parallel operations one for each key in the dataset.\r\n",
                " - Because datasets can have very large number of keys reduceByKey is not implemented as an action but as a transformation.\r\n",
                " - It returns a new RDD consisting of a key and the reduced value for that key\r\n",
                " - operates on key, value (k,v) pairs and merges the values for each key\r\n",
                "\r\n",
                "\r\n",
                "2. **groupByKey()**: Group all the values with the same key in the pair RDD.\r\n",
                "\r\n",
                "\r\n",
                "3. **sortByKey()**: Return an RDD sorted by the key\r\n",
                " - Sorting of data is necessary for many downstream applications\r\n",
                " - We can sort pair RDD as long as there is an ordering defined in the key.\r\n",
                " - Returns an RDD sorted by key in ascedning or descending order.\r\n",
                "\r\n",
                "4. **join()**: Join two pair RDDs based on their key\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "2bdaf121-ddc4-495d-895c-0b5837923aef"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Create RDD from tuple example\r\n",
                "my_tuple = [('Sam', 23), ('Mary', 34), ('Peter', 25)]\r\n",
                "pairRDD_tuple = sc.parallelize(my_tuple)\r\n",
                "\r\n",
                "# Create RDD from list\r\n",
                "my_list = ['Sam 23', 'Mary 34', 'Peter 25']\r\n",
                "regularRDD = sc.parallelize(my_list)\r\n",
                "pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0], s.split(' ')[1]))\r\n",
                "\r\n",
                "# reduceByKey Example\r\n",
                "regularRDD = sc.parallelize([(\"Messi\", 23), (\"Ronaldo\", 34),(\"Neymar\", 22), (\"Messi\", 24)])\r\n",
                "pairRDD_reducebykey = regularRDD.reduceByKey(lambda x,y : x + y)\r\n",
                "pairRDD_reducebykey.collect()\r\n",
                "\r\n",
                "# sortByKey Example\r\n",
                "pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))\r\n",
                "pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()\r\n",
                "\r\n",
                "# groupByKey Example\r\n",
                "airports = [(\"US\", \"JFK\"),(\"UK\", \"LHR\"),(\"FR\", \"CDG\"),(\"US\", \"SFO\")]\r\n",
                "regularRDD = sc.parallelize(airports)\r\n",
                "pairRDD_group = regularRDD.groupByKey().collect()\r\n",
                "for cont, air in pairRDD_group:\r\n",
                "      print(cont, list(air))\r\n",
                "\r\n",
                "# join example\r\n",
                "RDD1 = sc.parallelize([(\"Messi\", 34),(\"Ronaldo\", 32),(\"Neymar\", 24)])\r\n",
                "RDD2 = sc.parallelize([(\"Ronaldo\", 80),(\"Neymar\", 120),(\"Messi\", 100)])\r\n",
                "RDD1.join(RDD2).collect()"
            ],
            "metadata": {
                "azdata_cell_guid": "45043c97-8e92-400b-b124-8cfaa2ad9da7",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Create a pair RDD named Rdd with tuples (1,2),(3,4),(3,6),(4,5).\r\n",
                "\r\n",
                "Transform the Rdd with reduceByKey() into a pair RDD Rdd_Reduced by adding the values with the same key.\r\n",
                "\r\n",
                "Collect the contents of pair RDD Rdd_Reduced and iterate to print the output."
            ],
            "metadata": {
                "azdata_cell_guid": "1ceac2cc-931a-411b-9e09-05d254c631be"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Create PairRDD Rdd with key value pairs\r\n",
                "Rdd = sc.parallelize([(1,2), (3,4), (3,6), (4,5)])\r\n",
                "\r\n",
                "# Apply reduceByKey() operation on Rdd\r\n",
                "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)\r\n",
                "\r\n",
                "# Iterate over the result and print the output\r\n",
                "for num in Rdd_Reduced.collect(): \r\n",
                "  print(\"Key {} has {} Counts\".format(num[0], num[1]))"
            ],
            "metadata": {
                "azdata_cell_guid": "d097e2b6-15b3-4ef1-b25b-53b2b175b3b1",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "1. Sort the Rdd_Reduced RDD using the key in descending order.\r\n",
                "\r\n",
                "2. Collect the contents and iterate to print the output."
            ],
            "metadata": {
                "azdata_cell_guid": "859af5e7-2d8a-4ac8-b041-23c55d53685b"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Sort the reduced RDD with the key by descending order\r\n",
                "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\r\n",
                "\r\n",
                "# Iterate over the result and print the output\r\n",
                "for num in Rdd_Reduced_Sort.collect():\r\n",
                "  print(\"Key {} has {} Counts\".format(num[0], num[1]))"
            ],
            "metadata": {
                "azdata_cell_guid": "ae27fb69-97a7-4e77-a0ca-a6ebf8b394ee",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Advanced RDD Actions\r\n",
                "\r\n",
                "`Reduce` action takes in a function which operates on two elements of the same type of RDD and retruns a new element of the same type.\r\n",
                "- The function should be commutative(changing the order of the operands does not change the results) and associative so that it can be computed in parallel. Ex. `+` sign which we can use to sum our RDD.\r\n",
                "\r\n",
                "`saveAsTextFile`: In many cases it is not advisable to run collect action on RDDs because of the huge size of data. In these cases it is common to write out the data to distributed storage systems such as HDFS or Amazon S3.\r\n",
                "\r\n",
                "- saveAsTextFile can be used to save RDD as a text file inside a particular directory with each partition as a separate file(Example 1).By default, saveAsTextFile saves RDD with each partition as a separate file inside a directory. However you can change it to return a new RDD that is reduced to single partition using the `coalesce` method(Example 2). \r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "085ce65b-3167-4186-bd1f-1e2cd90990a1"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "x = [1,3,4,6]\r\n",
                "RDD = sc.parallelize(x)\r\n",
                "RDD.reduce(lambda x, y : x + y)\r\n",
                "\r\n",
                "# Example 1\r\n",
                "RDD.saveAsTextFile(\"tempFile\")\r\n",
                "\r\n",
                "# Example 2\r\n",
                "RDD.coalesce(1).saveAsTextFile(\"tempFile\")\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "bd918ff4-0ab4-402c-977b-8a50cd69f467"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "Similar to pair RDD Transformations, there are also RDD Actions available for pair RDDs. However, pair RDDs also attain some additional actions of PySpark especially those that leverage the advantage of data which is of key-value nature.\r\n",
                "\r\n",
                "`countByKey()` action countByKey() only available for type (K,V)    \r\n",
                "\r\n",
                " - countByKey action counts the number of elements for each key\r\n",
                " - One thing to note is that countByKey should only be used on a dataset whose size is small enough to fit in memory.\r\n",
                "\r\n",
                "`collectAsMap()` action returns the key-value pairs in the RDD to the as a dictionary. \r\n",
                "- Similar to countByKey, this action should only be used if the resulting data is expected to be small, as all the data is loaded into the memory.\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "3696788c-ed41-4d18-a5b1-57b9c1468f47"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# countByKey Example\r\n",
                "onasimplelistrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\r\n",
                " for kee, val in rdd.countByKey().items(): \r\n",
                "  print(kee, val)\r\n",
                "\r\n",
                "# collectAsMap Example\r\n",
                "sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\r\n",
                "\r\n",
                "\r\n",
                "# Example 2\r\n",
                "# Transform the rdd with countByKey()\r\n",
                "total = Rdd.countByKey()\r\n",
                "\r\n",
                "# What is the type of total?\r\n",
                "print(\"The type of total is\", type(total))\r\n",
                "\r\n",
                "# Iterate over the total and print the output\r\n",
                "for k, v in total.items(): \r\n",
                "  print(\"key\", k, \"has\", v, \"counts\")\r\n",
                "\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "99cfcc93-3c01-41f5-ae1d-a79dbda2357c",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "In this example, you will write code that calculates the most common words from Complete Works of William Shakespeare.\r\n",
                "\r\n",
                "Here are the brief steps for writing the word counting program:\r\n",
                "\r\n",
                "1. Create a base RDD from Complete_Shakespeare.txt file.\r\n",
                "2. Use RDD transformation to create a long list of words from each element of the base RDD.\r\n",
                "3. Remove stop words from your data.\r\n",
                "4. Create pair RDD where each element is a pair tuple of ('w', 1)\r\n",
                "5. Group the elements of the pair RDD by key (word) and add up their values.\r\n",
                "6. Swap the keys (word) and values (counts) so that keys is count and value is the word.\r\n",
                "7. Finally, sort the RDD by descending order and print the 10 most frequent words and their frequencies.\r\n",
                "\r\n",
                "- Create an RDD called baseRDD that reads lines from file_path.\r\n",
                "- Transform the baseRDD into a long list of words and create a new splitRDD.\r\n",
                "- Count the total words in splitRDD\r\n",
                "- Convert the words in splitRDD in lower case and then remove stop words from stop_words.\r\n",
                "- Create a pair RDD tuple containing the word and the number 1 from each word element in splitRDD.\r\n",
                "- Get the count of the number of occurrences of each word (word frequency) in the pair RDD using reduceByKey()\r\n",
                "- Print the first 10 words and their frequencies from the resultRDD.\r\n",
                "- Swap the keys and values in the resultRDD.\r\n",
                "- Sort the keys according to descending order.\r\n",
                "- Print the top 10 most frequent words and their frequencies.\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "a644568b-97ec-4d7f-9bd3-750fc781f136"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Create a baseRDD from the file path\r\n",
                "baseRDD = sc.textFile(file_path)\r\n",
                "\r\n",
                "# Split the lines of baseRDD into words\r\n",
                "splitRDD = baseRDD.flatMap(lambda x: x.split(\" \"))\r\n",
                "\r\n",
                "# Count the total number of words\r\n",
                "print(\"Total number of words in splitRDD:\", splitRDD.count())\r\n",
                "\r\n",
                "# Convert the words in lower case and remove stop words from stop_words\r\n",
                "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\r\n",
                "\r\n",
                "# Create a tuple of the word and 1 \r\n",
                "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\r\n",
                "\r\n",
                "# Count of the number of occurences of each word\r\n",
                "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)\r\n",
                "\r\n",
                "# Display the first 10 words and their frequencies\r\n",
                "for word in resultRDD.take(10):\r\n",
                "\tprint(word)\r\n",
                "\r\n",
                "# Swap the keys and values \r\n",
                "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\r\n",
                "\r\n",
                "# Sort the keys in descending order\r\n",
                "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\r\n",
                "\r\n",
                "# Show the top 10 most frequent words and their frequencies\r\n",
                "for word in resultRDD_swap_sort.take(10):\r\n",
                "\tprint(\"{} has {} counts\". format(word[1], word[0]))"
            ],
            "metadata": {
                "azdata_cell_guid": "c8066bc7-9f1c-4540-9c9b-c0f8de8667e6",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## PySpark SQL & DataFrames\r\n",
                "\r\n",
                "RDDs - Spark’s core abstraction for working with data.\r\n",
                "\r\n",
                "PySpark - Spark's high level API for working with structured data.\r\n",
                "\r\n",
                " - PySpark SQL is a Spark library for structured data. Unlike the PySpark RDD API, PySpark SQL provides more information about the structure of data and the computation being performed. \r\n",
                "\r\n",
                "- PySpark SQL provides a programming abstraction called DataFrames. A DataFrame is an immutable distributed collection of data with named columns. It is similar to a table in SQL. DataFrames are designed to process a large collection of structured data such as relational database and semi-structured data such as JSON (JavaScript Object Notation). DataFrame API currently supports several languages such as Python, R, Scala, and Java. DataFrames allows PySpark to query data using SQL, for example (SELECT * from table) or using the expression method for example (df.select()).\r\n",
                "\r\n",
                "\r\n",
                "- SparkContext which is the main entry point for creating RDDs. Similarly, `SparkSession` provides a single point of entry to interact with underlying Spark functionality and allows programming Spark with DataFrame API. The `SparkSession` does for DataFrames what the SparkContext does for RDDs.\r\n",
                "\r\n",
                "-  A `SparkSession` can be used to create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables etc., Similar to SparkContext, SparkSession is exposed to the PySpark shell as variable `spark`\r\n",
                "\r\n",
                "- Two different methods of creating DataFrames in PySpark \r\n",
                " - From existing RDDs using SparkSession's createDataFrame() method\r\n",
                "    -  When the schema is a list of column names, the type of each column will be inferred from data as shown above. However when the schema is None, it will try to infer the schema from data\r\n",
                " - From various data sources(CSV,JSON,TXT) using SparkSession's readmethod \r\n",
                "    - Arguments - Path to the file and two optional parameters \r\n",
                "     1. header=True\r\n",
                "     2. inferSchema=True\r\n",
                "- Schema controls the data (structure of data in the Dataframe) and helps Data Frames to optimize queries(Spark to optimize queries on data more efficiently)\r\n",
                "- Schema provides information about column name,type of data in the column,null or empty values etc.,\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "7dab4364-af58-43c9-894f-088bb7b88fa1"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Example using createDataFrame method\r\n",
                "iphones_RDD = sc.parallelize([(\"XS\", 2018, 5.65, 2.79, 6.24),\r\n",
                "                               (\"XR\", 2018, 5.94, 2.98, 6.84), \r\n",
                "                               (\"X10\", 2017, 5.65, 2.79, 6.13),\r\n",
                "                               (\"8Plus\", 2017, 6.23, 3.07, 7.12)])\r\n",
                "\r\n",
                "names = ['Model', 'Year', 'Height', 'Width', 'Weight']\r\n",
                "\r\n",
                "iphones_df = spark.createDataFrame(iphones_RDD, schema=names)\r\n",
                "\r\n",
                "type(iphones_df)\r\n",
                "\r\n",
                "# Example using read() method\r\n",
                "df_csv = spark.read.csv(\"people.csv\", header=True, inferSchema=True)\r\n",
                "df_json = spark.read.json(\"people.json\", header=True, inferSchema=True)\r\n",
                "df_txt = spark.read.txt(\"people.txt\", header=True, inferSchema=True)\r\n",
                "\r\n",
                " "
            ],
            "metadata": {
                "azdata_cell_guid": "18338235-8d78-4a15-93c3-634a92d8a514"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Create a list of tuples\r\n",
                "sample_list = [('Mona',20), ('Jennifer',34), ('John',20), ('Jim',26)]\r\n",
                "\r\n",
                "# Create a RDD from the list\r\n",
                "rdd = sc.parallelize(sample_list)\r\n",
                "\r\n",
                "# Create a PySpark DataFrame\r\n",
                "names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])\r\n",
                "\r\n",
                "# Check the type of names_df\r\n",
                "print(\"The type of names_df is\", type(names_df))\r\n",
                "\r\n",
                "# Create an DataFrame from file_path\r\n",
                "people_df = spark.read.csv(file_path, header=True, inferSchema=True)\r\n",
                "\r\n",
                "# Check the type of people_df\r\n",
                "print(\"The type of people_df is\", type(people_df))"
            ],
            "metadata": {
                "azdata_cell_guid": "0c17f093-380d-4330-a811-78bd379cd1ca",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Operating on DataFrames in PySpark\r\n",
                "\r\n",
                "Similar to RDD operations, the DataFrame operations in PySpark can be divided into Transformations and Actions. PySpark DataFrame provides operations to filter, group, or compute aggregates, and can be used with PySpark SQL.\r\n",
                "\r\n",
                "DataFrame Transformations : select(), lter(), groupby(), orderby(), dropDuplicates() and withColumnRenamed()\r\n",
                "\r\n",
                "DataFrame Actions : printSchema(), head(), show(), count(),columns and describe()\r\n",
                "\r\n",
                "`select()` transformation subsets the columns in the DataFrame \r\n",
                "\r\n",
                "`show()` action prints first 20 rows in the DataFrame\r\n",
                "\r\n",
                "`filter()` transformation filters out the rows based on a condition \r\n",
                "\r\n",
                "`groupby()` The groupby Transformation groups the DataFrame using the specified columns, so we can run aggregation on them\r\n",
                "\r\n",
                "`orderby()` operations sorts the DataFrame based one or more columns\r\n",
                "\r\n",
                "`dropDuplicates()` removes the duplicate rows of a DataFrame\r\n",
                "\r\n",
                "`withColumnRenamed()` renames a column in the DataFrame\r\n",
                "\r\n",
                "`printSchema()` operation prints the types of columns in the DataFrame\r\n",
                "\r\n",
                "`columns` operator prints the columns of a DataFrame\r\n",
                "\r\n",
                "`describe()` operation compute summary statistics of numerical columns in the DataFrame. If we don’t specify the name of columns it will calculate summary statistics for all numerical columns present in the DataFrame\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "0cb21ade-5741-4ee8-9f66-767353df2562"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Example 1 - select\r\n",
                "df_id_age = test.select('Age')\r\n",
                "\r\n",
                "# Example 2 - show\r\n",
                "df_id_age.show(3)\r\n",
                "\r\n",
                "# Example 3 - filter\r\n",
                "new_df_age21 = new_df.filter(new_df.Age > 21)\r\n",
                "new_df_age21.show(3)\r\n",
                "\r\n",
                "# Example 4 - groupby\r\n",
                "test_df_age_group = test_df.groupby('Age')\r\n",
                "test_df_age_group.count().show(3)\r\n",
                "\r\n",
                "# Example 5 -- orderBy\r\n",
                "test_df_age_group.count().orderBy('Age').show(3)\r\n",
                "\r\n",
                "# Example 6 - dropDuplicates\r\n",
                "test_df_no_dup = test_df.select('User_ID','Gender', 'Age').dropDuplicates()\r\n",
                "test_df_no_dup.count()\r\n",
                "\r\n",
                "# Example 7 - withColumnRenamed\r\n",
                "test_df_sex = test_df.withColumnRenamed('Gender', 'Sex')\r\n",
                "test_df_sex.show(3)\r\n",
                "\r\n",
                "# Example 8 - printSchema\r\n",
                "test_df.printSchema()\r\n",
                "\r\n",
                "# Example 9 columns\r\n",
                "test_df.columns\r\n",
                "\r\n",
                "# Example 10 - describe\r\n",
                "test_df.describe().show()"
            ],
            "metadata": {
                "azdata_cell_guid": "a7db2d72-fc51-4eee-9846-a04a7d0d4c2d"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Inspecting data in PySpark DataFrame\r\n",
                "\r\n",
                "Print the first 10 observations in the people_df DataFrame.\r\n",
                "\r\n",
                "Count the number of rows in the people_df DataFrame.\r\n",
                "\r\n",
                "How many columns does people_df DataFrame have and what are their names?"
            ],
            "metadata": {
                "azdata_cell_guid": "12853912-7ea3-4212-b60f-51a12e50e240"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Print the first 10 observations \r\n",
                "people_df.show(10)\r\n",
                "\r\n",
                "# Count the number of rows \r\n",
                "print(\"There are {} rows in the people_df DataFrame.\".format(people_df.count()))\r\n",
                "\r\n",
                "# Count the number of columns and their names\r\n",
                "print(\"There are {} columns in the people_df DataFrame and their names are {}\".format(len(people_df.columns), people_df.columns))"
            ],
            "metadata": {
                "azdata_cell_guid": "4f9e4d3f-c0e3-478c-b90a-6b694507a0ce"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### PySpark DataFrame subsetting and cleaning\r\n",
                "\r\n",
                "Select 'name', 'sex' and 'date of birth' columns from people_df and create people_df_sub DataFrame.\r\n",
                "\r\n",
                "Print the first 10 observations in the people_df DataFrame.\r\n",
                "\r\n",
                "Remove duplicate entries from people_df_sub DataFrame and create people_df_sub_nodup DataFrame.\r\n",
                "\r\n",
                "How many rows are there before and after duplicates are removed?"
            ],
            "metadata": {
                "azdata_cell_guid": "94293b40-f79b-4a63-9e96-38215aad84b0"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Select name, sex and date of birth columns\r\n",
                "people_df_sub = people_df.select('name', 'sex', 'date of birth')\r\n",
                "\r\n",
                "# Print the first 10 observations from people_df_sub\r\n",
                "people_df_sub.show(10)\r\n",
                "\r\n",
                "# Remove duplicate entries from people_df_sub\r\n",
                "people_df_sub_nodup = people_df_sub.dropDuplicates()\r\n",
                "\r\n",
                "# Count the number of rows\r\n",
                "print(\"There were {} rows before removing duplicates, and {} rows after removing duplicates\".format(people_df_sub.count(),people_df_sub_nodup.count()))\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "14fce6eb-d035-4d8c-851c-fa734dfa7fe2",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Filtering your DataFrame\r\n",
                "\r\n",
                "Filter the people_df DataFrame to select all rows where sex is female into people_df_female DataFrame.\r\n",
                "\r\n",
                "Filter the people_df DataFrame to select all rows where sex is male into people_df_male DataFrame.\r\n",
                "\r\n",
                "Count the number of rows in people_df_female and people_df_male DataFrames."
            ],
            "metadata": {
                "azdata_cell_guid": "0efc7f82-243e-4c63-a22e-44aa8e09219f"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Filter people_df to select females \r\n",
                "people_df_female = people_df.filter(people_df.sex == \"female\")\r\n",
                "\r\n",
                "# Filter people_df to select males\r\n",
                "people_df_male = people_df.filter(people_df.sex == \"male\")\r\n",
                "\r\n",
                "# Count the number of rows \r\n",
                "print(\"There are {} rows in the people_df_female DataFrame and {} rows in the people_df_male DataFrame\".format(people_df_female.count(), people_df_male.count()))"
            ],
            "metadata": {
                "azdata_cell_guid": "fce44ea6-489b-4f36-badb-1e539d12d9ec"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Interacting with DataFrames using PySpark SQL\r\n",
                "\r\n",
                "- In addition to DataFrame API, PySpark SQL allows you to manipulate DataFrames with SQL queries. What you can do using DataFrames API, can be done using SQL queries and vice versa.\r\n",
                "- The DataFrames API provides a programmatic interface – basically a domain-specific language (DSL) for interacting with data.\r\n",
                "- DataFrame queries are much easier to construct programmatically. Plain SQL queries can be significantly more concise and easier to understand. They are also portable and can be used without any modifications with every supported language. Many of the DataFrame operations that you have seen in the previous chapter, can be done using SQL queries.\r\n",
                "- The SparkSession provides a method called sql which can be used to execute a SQL query. The sql method takes a SQL statement as an argument and returns a DataFrame representing the result of the given query.\r\n",
                "-  Unfortunately, SQL queries cannot be run directly against a DataFrame. To issue SQL queries against an existing DataFrame we can leverage the `createOrReplaceTempView` function to build a temporary table as shown in this example.\r\n",
                "-  After creating the temporary table, we can simply use the sql method, which allows us to write SQL code to manipulate data within a DataFrame.Since the result is a DataFrame, you can run DataFrame actions such as collect, first, show etc.\r\n",
                "- The SQL queries are not limited to extracting data, We can also create SQL queries to run aggregations\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "352ea0aa-31de-4d33-bb55-64176da06ccc"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Example 1\r\n",
                "df.createOrReplaceTempView(\"table1\")\r\n",
                "df2 = spark.sql(\"SELECT field1, field2 FROM table1\")\r\n",
                "df2.collect()\r\n",
                "\r\n",
                "# Example 2\r\n",
                "test_df.createOrReplaceTempView(\"test_table\")\r\n",
                "query = '''SELECT Product_ID FROM test_table'''\r\n",
                "test_product_df = spark.sql(query)\r\n",
                "test_product_df.show(5)\r\n",
                "\r\n",
                "# Example 3 - Aggregations\r\n",
                "test_df.createOrReplaceTempView(\"test_table\")\r\n",
                "query = '''SELECT Age, max(Purchase) FROM test_table GROUP BY Age'''\r\n",
                "spark.sql(query).show(5)\r\n",
                "\r\n",
                "# Example 4 - Filtering\r\n",
                "test_df.createOrReplaceTempView(\"test_table\")\r\n",
                "query = '''SELECT Age, Purchase, Gender FROM table1 WHERE Purchase > 20000 AND Gender == \"F\"'''\r\n",
                "spark.sql(query).show(5)"
            ],
            "metadata": {
                "azdata_cell_guid": "e81b0cf5-c3f3-4db2-99c4-ad08b3a5d74a"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Running SQL Queries Programmatically\r\n",
                "\r\n",
                "Create a temporary table people that's a pointer to the people_df DataFrame.\r\n",
                "\r\n",
                "Construct a query to select the names of the people from the temporary table people.\r\n",
                "\r\n",
                "Assign the result of Spark's query to a new DataFrame - people_df_names.\r\n",
                "\r\n",
                "Print the top 10 names of the people from people_df_names DataFrame."
            ],
            "metadata": {
                "azdata_cell_guid": "0a8b09a3-2583-409a-87dd-ead864c5b0c1"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Create a temporary table \"people\"\r\n",
                "people_df.createOrReplaceTempView(\"people\")\r\n",
                "\r\n",
                "# Construct a query to select the names of the people from the temporary table \"people\"\r\n",
                "query = '''SELECT name FROM people'''\r\n",
                "\r\n",
                "# Assign the result of Spark's query to people_df_names\r\n",
                "people_df_names = spark.sql(query)\r\n",
                "\r\n",
                "# Print the top 10 names of the people\r\n",
                "people_df_names.show(10)"
            ],
            "metadata": {
                "azdata_cell_guid": "c4ad5e8d-5f07-4869-a76d-789ca7e2eab9"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### SQL queries for filtering Table\r\n",
                "\r\n",
                "1. Filter the people table to select all rows where sex is female into people_female_df DataFrame.\r\n",
                "2. Filter the people table to select all rows where sex is male into people_male_df DataFrame.\r\n",
                "3. Count the number of rows in both people_female and people_male DataFrames"
            ],
            "metadata": {
                "azdata_cell_guid": "a81c37e5-3586-4c5c-9697-8a14f9e95fd2"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Filter the people table to select female sex \r\n",
                "people_female_df = spark.sql('SELECT * FROM people WHERE sex==\"female\"')\r\n",
                "\r\n",
                "# Filter the people table DataFrame to select male sex\r\n",
                "people_male_df = spark.sql('SELECT * FROM people WHERE sex==\"male\"')\r\n",
                "\r\n",
                "# Count the number of rows in both DataFrames\r\n",
                "print(\"There are {} rows in the people_female_df and {} rows in the people_male_df DataFrames\".format(people_female_df.count(), people_male_df.count()))"
            ],
            "metadata": {
                "azdata_cell_guid": "3c88b62e-e55d-4801-90b5-683a80a92cda",
                "tags": [
                    "hide_input"
                ]
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Data Visualization in PySpark using DataFrames\r\n",
                "\r\n",
                "Ploing graphs using PySpark DataFrames is done using three methods\r\n",
                "- pyspark_dist_explore library \r\n",
                "  - Pyspark_dist_explore library provides quick insights into DataFrames \r\n",
                "  - Currently three functions available – `hist()`, `distplot()` and `pandas_histogram()` to create matplotlib graphs while minimizing the amount of computation needed\r\n",
                "- toPandas() method converts the PySpark DataFrame into a Pandas DataFrame after conversion It's easy to create charts from pandas DataFrames using matplotlib or seaborn plotting tools\r\n",
                "   - Pandas DataFrame vs PySpark DataFrame \r\n",
                "    - PandasvDataFrames are in-memory single-server based(single machine tools constrained by single machine limits) whereas structures and operations on PySpark run in parallel on different nodes in a cluster\r\n",
                "    - The result is generated as we apply any operation in Pandas whereas operations in PySpark DataFrame are lazy evaluation \r\n",
                "    - Pandas Data Frame as mutable and PySpark DataFrames are immutable     \r\n",
                "    - Pandas API support more operations than PySpark Dataframe API\r\n",
                "- HandySpark library\r\n",
                "   - HandySpark is a package designed to improve PySpark user experience\r\n",
                "   - It makes fetching data or computing statistics for columns really easy, returning pandas objects straight away.\r\n",
                "   - It brings the long-missing capability of plotting data while retaining the advantage of performing the distributed computation\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "74c9217e-f119-4b12-91a7-a9851c7c4199"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Example 1 - Using pyspark_dist_explore library\r\n",
                "test_df = spark.read.csv(\"test.csv\", header=True, inferSchema=True)\r\n",
                "test_df_age = test_df.select('Age')\r\n",
                "hist(test_df_age, bins=20, color=\"red\")\r\n",
                "\r\n",
                "# Example 2 - Using toPandas\r\n",
                "test_df = spark.read.csv(\"test.csv\", header=True, inferSchema=True)\r\n",
                "test_df_sample_pandas = test_df_sample.toPandas()\r\n",
                "test_df_sample_pandas.hist('Age')\r\n",
                "\r\n",
                "#Example 3 - HandySpark\r\n",
                "test_df = spark.read.csv('test.csv', header=True, inferSchema=True)\r\n",
                "hdf = test_df.toHandy()\r\n",
                "hdf.cols[\"Age\"].hist()\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "4fc8b1ee-2532-4e0d-b9c3-8eb7dd8d1323"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### PySpark DataFrame visualization\r\n",
                "\r\n",
                "- Print the names of the columns in names_df DataFrame.\r\n",
                "- Convert names_df DataFrame to df_pandas Pandas DataFrame.\r\n",
                "- Use matplotlib's plot() method to create a horizontal bar plot with 'Name' on x-axis and 'Age' on y-axis."
            ],
            "metadata": {
                "azdata_cell_guid": "b4147456-140f-4cc0-8d3d-4a4ccef43523"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Check the column names of names_df\r\n",
                "print(\"The column names of names_df are\", names_df.columns)\r\n",
                "\r\n",
                "# Convert to Pandas DataFrame  \r\n",
                "df_pandas = names_df.toPandas()\r\n",
                "\r\n",
                "# Create a horizontal bar plot\r\n",
                "df_pandas.plot(kind='barh', x='Name', y='Age', colormap='winter_r')\r\n",
                "plt.show()"
            ],
            "metadata": {
                "azdata_cell_guid": "a3263022-79d0-455c-a3e4-1ff6876c0140"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Part 1: Create a DataFrame from CSV file\r\n",
                "\r\n",
                "Create a PySpark DataFrame from file_path which is the path to the Fifa2018_dataset.csv file.\r\n",
                "\r\n",
                "Print the schema of the DataFrame.\r\n",
                "\r\n",
                "Print the first 10 observations.\r\n",
                "\r\n",
                "How many rows are in there in the DataFrame?"
            ],
            "metadata": {
                "azdata_cell_guid": "9f570669-4079-480a-b008-dcb1d19c91b6"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Load the Dataframe\r\n",
                "fifa_df = spark.read.csv(file_path, header=True, inferSchema=True)\r\n",
                "\r\n",
                "# Check the schema of columns\r\n",
                "fifa_df.printSchema()\r\n",
                "\r\n",
                "# Show the first 10 observations\r\n",
                "fifa_df.show(10)\r\n",
                "\r\n",
                "# Print the total number of rows\r\n",
                "print(\"There are {} rows in the fifa_df DataFrame\".format(fifa_df.count()))"
            ],
            "metadata": {
                "azdata_cell_guid": "2f51e34e-2f9d-4185-bbc8-5c69cff4e241"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Part 2: SQL Queries on DataFrame\r\n",
                "\r\n",
                "Create temporary table fifa_df from fifa_df_table DataFrame.\r\n",
                "\r\n",
                "Construct a \"query\" to extract the \"Age\" column from Germany players.\r\n",
                "\r\n",
                "Apply the SQL \"query\" to the temporary view table and create a new DataFrame.\r\n",
                "\r\n",
                "Computes basic statistics of the created DataFrame."
            ],
            "metadata": {
                "azdata_cell_guid": "fa41bf31-c0fd-46f7-8476-f5e97da6e743"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Create a temporary view of fifa_df\r\n",
                "fifa_df.createOrReplaceTempView('fifa_df_table')\r\n",
                "\r\n",
                "# Construct the \"query\"\r\n",
                "query = '''SELECT Age FROM fifa_df_table WHERE Nationality == \"Germany\"'''\r\n",
                "\r\n",
                "# Apply the SQL \"query\"\r\n",
                "fifa_df_germany_age = spark.sql(query)\r\n",
                "\r\n",
                "# Generate basic statistics\r\n",
                "fifa_df_germany_age.describe().show()"
            ],
            "metadata": {
                "azdata_cell_guid": "c8c538ad-981f-4d0b-92c5-d65968d3f905"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Part 3: Data visualization\r\n",
                "\r\n",
                "Convert fifa_df_germany_age to fifa_df_germany_age_pandas Pandas DataFrame.\r\n",
                "\r\n",
                "Generate a density plot of the 'Age' column from the fifa_df_germany_age_pandas Pandas DataFrame."
            ],
            "metadata": {
                "azdata_cell_guid": "0e3d314f-9949-4c27-b7f5-59302d115f2c"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Convert fifa_df to fifa_df_germany_age_pandas DataFrame\r\n",
                "fifa_df_germany_age_pandas = fifa_df_germany_age.toPandas()\r\n",
                "\r\n",
                "# Plot the 'Age' density of Germany Players\r\n",
                "fifa_df_germany_age_pandas.plot(kind='density')\r\n",
                "plt.show()"
            ],
            "metadata": {
                "azdata_cell_guid": "351495e8-43ba-4cac-8aa4-02d5b391073b"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}