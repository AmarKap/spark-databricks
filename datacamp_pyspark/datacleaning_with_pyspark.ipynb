{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Data Cleaning with Apache Spark\r\n",
                "\r\n",
                "- Data cleaning is preparing raw data for use in processing pipelines\r\n",
                "- Data cleaning is a necessary part of any production data system. If your data isn't \"clean\", it's not trustworthy and could cause problems later on.\r\n",
                "- There are many tasks that could fall under the data cleaning umbrella. A few of these include reformatting or replacing text; performing calculations based on the data; and removing garbage or incomplete data.\r\n",
                "- Most data cleaning systems have two big problems: optimizing performance and organizing the flow of data.\r\n",
                "-  Spark lets you scale your data processing capacity as your requirements evolve. Beyond the performance issues, dealing with large quantities of data requires a process, or pipeline of steps. Spark allows management of many complex tasks within a single framework.\r\n",
                "\r\n",
                "## Spark Schemas\r\n",
                "- A primary function of data cleaning is to verify all data is in the expected format. Spark provides a built-in ability to validate datasets with schemas.\r\n",
                "-  You may have used schemas before with databases or XML; Spark is similar. A schema defines and validates the number and types of columns for a given DataFrame. A schema can contain many different types of fields - integers, floats, dates, strings, and even arrays or mapping structures.\r\n",
                "-  A defined schema allows Spark to `filter` out data that doesn't conform during read, `ensuring expected correctness`. In addition, schemas also have `performance benefits`.\r\n",
                "-  Normally a data import will try to infer a schema on read - this requires reading the data twice. Defining a schema limits this to a single read operation."
            ],
            "metadata": {
                "azdata_cell_guid": "c2a7e7cf-e89f-4288-a742-3442a339533f"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# import pyspark.sql.types\r\n",
                "from pyspark.sql.types import *\r\n",
                "\r\n",
                "peopleSchema = StructType([\r\n",
                "    # Define the name field\r\n",
                "    StructField('name', StringType(), True),\r\n",
                "    # Add the age field\r\n",
                "    StructField('age', IntegerType(), True),\r\n",
                "    # Add the city field\r\n",
                "    StructField('city', StringType(), True)\r\n",
                " ])\r\n",
                "\r\n",
                "# Read CSV containing data\r\n",
                "people_df = spark.read.format('csv').load(name='rawdata.csv', schema=peopleSchema)\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "3db752f3-86df-4853-accb-8ef00e5f9e07"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Immutability and lazy processing\r\n",
                "\r\n",
                "- Python variables are fully mutable. The values can be changed at any given time, assuming the scope of the variable is valid.\r\n",
                "- While very flexible, this does present problems anytime there are multiple concurrent components trying to modify the same data. Most languages work around these issues using constructs like mutexes, semaphores, etc. This can add complexity, especially with non-trivial programs.\r\n",
                "- Spark Data Frames are immutable,  this means Spark Data Frames are defined once and are not modifiable after initialization (Unable to be directly modified). Immutability is a concept of functional programming and Spark is designed ot use immutable objects.\r\n",
                "- If the variable name is reused, the original data is removed (assuming it's not in use elsewhere) and the variable name is reassigned to the new data.(Re-created if reassigned)\r\n",
                "-  While this seems inefficient, it actually allows Spark to share data between all cluster components. It can do so without worry about concurrent data objects.(Able to be shared efficiently)"
            ],
            "metadata": {
                "azdata_cell_guid": "e122096b-631b-4302-86d9-ebca802d716b"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Define a new dataframe:\r\n",
                "voter_df = spark.read.csv('voterdata.csv')\r\n",
                "\r\n",
                "# Making changes: This does not modfiy the original data frame but it actually copeis the orignal definition\r\n",
                "# Adds/Applies the transformation and assigns it to the variable.\r\n",
                "voter_df = voter_df.withColumn('fullyear',voter_df.year + 2000)\r\n",
                "\r\n",
                "# Drop the column year\r\n",
                "voter_df = voter_df.drop(voter_df.year)\r\n",
                "\r\n",
                "# Action processes all transformations\r\n",
                "voter_df.count()\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "4866260c-b4c6-4c5c-9dc1-e94621bcde86"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Lazy Processing\r\n",
                "- Very little happens until an action is performed\r\n",
                "- When Spark works with big data the trick that it uses is that no data was actually read / added / modified, only thing gets updated are the instructions (aka, Transformations) for what we wanted Spark to do.This functionality allows Spark to perform the most efficient set of operations to get the desired result\r\n",
                "- Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. Remember that this is due to Spark not performing any transformations until an action is requested.\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "8ada8b4b-31e4-4b68-9fef-ef378f2571dd"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Load the CSV file\r\n",
                "aa_dfw_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2018.csv.gz')\r\n",
                "\r\n",
                "# Add the airport column using the F.lower() method\r\n",
                "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\r\n",
                "\r\n",
                "# Drop the Destination Airport column\r\n",
                "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\r\n",
                "\r\n",
                "# Show the DataFrame\r\n",
                "aa_dfw_df.show()"
            ],
            "metadata": {
                "azdata_cell_guid": "5c67a15d-03cc-4736-8c5d-afcdcedcd830"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Understanding Parquet\r\n",
                "\r\n",
                " - No defined schema - there are no data types included, nor column names (beyond a header row)\r\n",
                " - Nested data requires special handling - Using content containing a comma (or another delimiter) requires escaping. Using the escape character within content requires even further escaping\r\n",
                " -  Encoding format limited - The available encoding formats are limited depending on the language used.\r\n",
                "\r\n",
                " - Spark has some specific problems processing CSV data.\r\n",
                "    -  CSV files are quite slow to import and parse.\r\n",
                "    -  The files cannot be shared between workers during the import process.\r\n",
                "    -  If no schema is defined, all data must be read before a schema can be inferred.\r\n",
                "    - Spark has feature known as predicate pushdown. Basically, this is the idea of ordering tasks to do the least amount of work. Filtering data prior to processing is one of the primary optimizations of predicate pushdown. This drastically reduces the amount of information that must be processed in large data sets. Unfortunately, you cannot filter the CSV data via `predicate pushdown`\r\n",
                "    - Finally, Spark processes are often multi-step and may utilize an intermediate file representation. These representations allow data to be used later without regenerating the data from source. Using CSV would instead require a significant amount of extra work defining schemas, encoding formats, etc.\r\n",
                "\r\n",
                "- `Parquet` is a compressed columnar data format developed for use in any Hadoop based system. This includes Spark, Hadoop, Apache Impala, and so forth.\r\n",
                "- The Parquet format is structured with data accessible in chunks, allowing efficient read / write operations without processing the entire file. This structured format supports Spark's predicate pushdown functionality, providing significant performance improvement.\r\n",
                "- Parquet files automatically include schema information and handle data encoding. This is perfect for intermediary or on-disk representation of processed data. Note that Parquet files are a binary file format and can only be used with the proper tools. This is in contrast to CSV files which can be edited with any text editor.\r\n",
                "\r\n",
                "`Note`\r\n",
                "The Parquet format is a columnar data store, allowing Spark to use predicate pushdown. This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets."
            ],
            "metadata": {
                "azdata_cell_guid": "b1251ed7-b0f4-457b-b9b8-46458b6ccba3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Reading Parquet files\r\n",
                "# The long-form versions of each permit extra option flags, such as when overwriting an existing parquet file.\r\n",
                "df = spark.read.format('parquet').load('filename.parquet')\r\n",
                "df = spark.read.parquet('filename.parquet')\r\n",
                "\r\n",
                "# Writing Parquet files\r\n",
                "df.write.format('parquet').save('filename.parquet')\r\n",
                "df.write.parquet('filename.parquet')\r\n",
                "\r\n",
                "# Parquet perfect for performing SQL operations or using it as backing stores for SparkSQL operations \r\n",
                "# we get all the performance benefits primarily defined schemas and the available use of predicate pushdown\r\n",
                "flight_df = spark.read.parquet('flights.parquet')\r\n",
                "flight_df.createOrReplaceTempView('flights')\r\n",
                "short_flights_df = spark.sql('SELECT * FROM flights WHERE flightduration < 100')"
            ],
            "metadata": {
                "azdata_cell_guid": "1b8b079a-5ee3-44c2-ae7a-5ab85e8e7b83"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "\r\n",
                "# View the row count of df1 and df2\r\n",
                "print(\"df1 Count: %d\" % df1.count())\r\n",
                "print(\"df2 Count: %d\" % df2.count())\r\n",
                "\r\n",
                "# Combine the DataFrames into one\r\n",
                "df3 = df1.union(df2)\r\n",
                "\r\n",
                "# Save the df3 DataFrame in Parquet format\r\n",
                "df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\r\n",
                "\r\n",
                "# Read the Parquet file into a new DataFrame and run a count\r\n",
                "print(spark.read.parquet('AA_DFW_ALL.parquet').count())"
            ],
            "metadata": {
                "azdata_cell_guid": "1a4519b0-a8a9-49ab-90ea-e71f32f96bac"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Read the Parquet file into flights_df\r\n",
                "flights_df = spark.read.parquet('AA_DFW_ALL.parquet')\r\n",
                "\r\n",
                "# Register the temp table\r\n",
                "flights_df.createOrReplaceTempView('flights')\r\n",
                "\r\n",
                "# Run a SQL query of the average flight duration\r\n",
                "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\r\n",
                "print('The average flight time is: %d' % avg_duration)"
            ],
            "metadata": {
                "azdata_cell_guid": "8ff72091-79f9-4f4e-b9ed-388733dd97f4"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Spark Column Operations\n",
                "\n",
                "- DataFrames are made up of rows & columns and are generally analogous to a database table.\n",
                "- DataFrames are immutable: any change to the structure or content of the data creates a new DataFrame.\n",
                "- DataFrames are modified through the use of transformations.\n",
                "- Negate with `~`\n",
                "- Filtering includes only rows that satisfy the requirements defined in the argument."
            ],
            "metadata": {
                "azdata_cell_guid": "14e6d6f2-b277-4102-92f9-62e73e69e07c"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Return rows where name starts with \"M\" \r\n",
                "voter_df.filter(voter_df.name.like('M%'))\r\n",
                "\r\n",
                "# Return name and position only\r\n",
                "voters = voter_df.select('name', 'position')\r\n",
                "\r\n",
                "# Filter/Where\r\n",
                "voter_df.filter(voter_df.date > '1/1/2019') # or voter_df.where(...)\r\n",
                "\r\n",
                "# Select\r\n",
                "voter_df.select(voter_df.name)\r\n",
                "\r\n",
                "# withColumn\r\n",
                "voter_df.withColumn('year', voter_df.date.year)\r\n",
                "\r\n",
                "# drop Column\r\n",
                "voter_df.drop('unused_column')\r\n",
                "\r\n",
                "# Remove/Keep nulls\r\n",
                "voter_df.filter(voter_df['name'].isNotNull())\r\n",
                "voter_df.where(~ voter_df._c1.isNull())\r\n",
                "\r\n",
                "# Remove Odd Entries\r\n",
                "voter_df.filter(voter_df.date.year > 1800)\r\n",
                "voter_df.where(voter_df['_c0'].contains('VOTE'))\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "abbd543e-d2a5-4413-a940-346e9a9cee57"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Column String Transformations\r\n",
                "import pyspark.sql.functions as F\r\n",
                "\r\n",
                "# Applied per column as transformation\r\n",
                "voter_df.withColumn('upper', F.upper('name'))\r\n",
                "\r\n",
                "# Can create intermediary columns \r\n",
                "voter_df.withColumn('splits', F.split('name', ' '))\r\n",
                "\r\n",
                "# Can cast to other types\r\n",
                "voter_df.withColumn('year', voter_df['_c4'].cast(IntegerType()))"
            ],
            "metadata": {
                "azdata_cell_guid": "a6e5fc5f-4faf-4583-9aad-fa4430f9a868"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Array Type Column Functions\r\n",
                "\r\n",
                "- While performing data cleaning with Spark, you may need to interact with `ArrayType() `columns. These are analogous to lists in normal python environments. \r\n",
                "- Functions we will use are\r\n",
                "   1. `.size()`, which returns the number of items present in the specified ArrayType() argument. \r\n",
                "   2. `.getItem()`. It takes an index argument and returns the item present at that index in the list column."
            ],
            "metadata": {
                "azdata_cell_guid": "dccf229d-67ed-4af7-b096-5546fc52530f"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Show the distinct VOTER_NAME entries\r\n",
                "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\r\n",
                "\r\n",
                "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\r\n",
                "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\r\n",
                "\r\n",
                "# Filter out voter_df where the VOTER_NAME contains an underscore\r\n",
                "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\r\n",
                "\r\n",
                "# Show the distinct VOTER_NAME entries again\r\n",
                "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\r\n",
                "\r\n",
                "# Select Name & State column for ID greater than 3000\r\n",
                "users_df.filter('ID > 3000').select(\"Name\", \"State\")"
            ],
            "metadata": {
                "azdata_cell_guid": "70c56fe4-58f0-40ba-9d28-32a16818daed"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Add a new column called splits separated on whitespace\r\n",
                "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\r\n",
                "\r\n",
                "# Create a new column called first_name based on the first item in splits\r\n",
                "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\r\n",
                "\r\n",
                "# Get the last entry of the splits list and create a column called last_name\r\n",
                "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))\r\n",
                "\r\n",
                "# Drop the splits column\r\n",
                "voter_df = voter_df.drop('splits')\r\n",
                "\r\n",
                "# Show the voter_df DataFrame\r\n",
                "voter_df.show()"
            ],
            "metadata": {
                "azdata_cell_guid": "c18a7b3c-3b7d-4035-afef-29d255225753",
                "tags": []
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Conditional Operations in a Dataframe\r\n",
                "\r\n",
                "The DataFrame transformations we've covered thus far are blanket transformations, meaning they're applied regardless of the data.\r\n",
                "\r\n",
                "When you want to conditionally change some aspect of the contents. Spark provides some built in conditional clauses which act similar to an if / then / else statement in a traditional programming environment. While it is possible to perform a traditional if / then / else style statement in Spark, it can lead to serious performance degradation as each row of a DataFrame would be evaluated independently. Using the optimized, built-in conditionals alleviates this. There are two components to the conditional clauses: `.when()`, and the optional `.otherwise()`.\r\n",
                "\r\n",
                "The .when() clause is a method available from the pyspark.sql.functions library\r\n",
                "\r\n",
                "Syntax : `when(<if condition>, <then x>)`\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "948cc150-7aa7-4675-824e-0439ad3aa19e"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Ex. 1\r\n",
                "df.select(df.Name, df.Age, F.when(df.Age >= 18, \"Adult\"))\r\n",
                "\r\n",
                "# Ex. 2\r\n",
                "df.select(df.Name, df.Age,           \r\n",
                "          .when(df.Age >= 18, \"Adult\")\r\n",
                "          .when(df.Age < 18, \"Minor\"))\r\n",
                "\r\n",
                "# Ex. 3\r\n",
                "df.select(df.Name, df.Age,          \r\n",
                "          .when(df.Age >= 18, \"Adult\") \r\n",
                "          .otherwise(\"Minor\"))\r\n",
                "\r\n",
                "# Add a column to voter_df for any voter with the title **Councilmember**\r\n",
                "voter_df = voter_df.withColumn('random_val',\r\n",
                "                               when(voter_df.TITLE == 'Councilmember', F.rand()))\r\n",
                "\r\n",
                "# Show some of the DataFrame rows, noting whether the when clause worked\r\n",
                "voter_df.show()\r\n",
                "\r\n",
                "# Add a column to voter_df for a voter based on their position\r\n",
                "voter_df = voter_df.withColumn('random_val',\r\n",
                "                               when(voter_df.TITLE == 'Councilmember', F.rand())\r\n",
                "                               .when(voter_df.TITLE == 'Mayor', 2)\r\n",
                "                               .otherwise(0))\r\n",
                "\r\n",
                "# Show some of the DataFrame rows\r\n",
                "voter_df.show()\r\n",
                "\r\n",
                "# Use the .filter() clause with random_val\r\n",
                "voter_df.filter(voter_df.random_val==0).show()"
            ],
            "metadata": {
                "azdata_cell_guid": "8838f2ba-60b5-4e60-9e71-3f8ac767713b"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## User Defined Functions\r\n",
                "\r\n",
                "- A user defined function, or UDF, is a Python method that the user writes to perform a specific bit of logic.\r\n",
                "- Once written, the method is called via the pyspark.sql.functions.udf() method. The result is stored as a variable and can be called as a normal Spark function."
            ],
            "metadata": {
                "azdata_cell_guid": "a3f24043-1668-455e-ad08-a60e08ca9edf"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Define a Python method \r\n",
                "def reverseString(mystr):\r\n",
                "    return mystr[::-1]\r\n",
                "    \r\n",
                "# Wrap the function and store as a variable\r\n",
                "# Method takes two arguments one is the function name and second is the data type of the return object\r\n",
                "# This can be any of the options in pyspark.sql.types, and can even be a more complex type, including a fully defined schema object.\r\n",
                "# Most often, you'll return either a simple object type, or perhaps an ArrayType\r\n",
                "udfReverseString = udf(reverseString, StringType())\r\n",
                "\r\n",
                "# Use with Spark\r\n",
                "user_df = user_df.withColumn('ReverseName', udfReverseString(user_df.Name))\r\n",
                "\r\n",
                "# Argument less example\r\n",
                "def sortingCap():\r\n",
                "    return random.choice(['G', 'H', 'R', 'S'])\r\n",
                "\r\n",
                "udfSortingCap = udf(sortingCap, StringType())\r\n",
                "\r\n",
                "user_df = user_df.withColumn('Class', udfSortingCap())\r\n",
                "\r\n",
                "\r\n",
                "def getFirstAndMiddle(names):\r\n",
                "  # Return a space separated string of names\r\n",
                "  return ' '.join(names[:-1])\r\n",
                "\r\n",
                "# Define the method as a UDF\r\n",
                "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\r\n",
                "\r\n",
                "# Create a new column using your UDF\r\n",
                "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\r\n",
                "\r\n",
                "# Show the DataFrame\r\n",
                "voter_df.show()\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "4a24715e-6443-4527-80d3-c573842d94f3"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Partitioning and lazy processing\r\n",
                "\r\n",
                "- `Spark breaks DataFrames into partitions, or chunks of data.` These partitions can be automatically defined, enlarged, shrunk, and can differ greatly based on the type of Spark cluster being used. \r\n",
                "\r\n",
                "- The size of the partition does vary, but generally try to keep your partition sizes equal.Each partition is handled independently. This is part of what provides the performance levels and horizontal scaling ability in Spark. If a Spark node doesn't need to compete for resources, nor consult with other Spark nodes for answers, it can reliably schedule the processing for the best performance.\r\n",
                "\r\n",
                "- In Spark, any transformation operation is `lazy`; it's more like a recipe than a command. `It defines what should be done to a DataFrame rather than actually doing it`.\r\n",
                "\r\n",
                "- Most operations in Spark are actually `transformations`, including .withColumn(), .select(), .filter(), and so forth. The set of transformations you define are only executed when you run a Spark `action`.This includes .count(), .write(), etc - anything that requires the transformations to be run to properly obtain an answer. \r\n",
                "\r\n",
                "-  Spark can reorder transformations for the best performance. Usually this isn't noticeable, but can occasionally cause unexpected behavior, such as IDs not being added until after other transformations have completed. This doesn't actually cause a problem but the data can look unusual if you don't know what to expect.\r\n",
                "\r\n",
                "## Adding IDs\r\n",
                "\r\n",
                "- Relational databases tend to have a field used to identify the row, whether it is for an actual relationship reference, or just for data identification. These IDs are typically an integer that increases in value, is sequential, and most importantly unique.\r\n",
                "-  The problem with these IDs is they're not very parallel in nature. Given that the values are given out sequentially, if there are multiple workers, they must all refer to a common source for the next entry. This is OK in a single server environment, but in a distributed platform such as Spark, it creates some undue bottlenecks.\r\n",
                "- Spark has a built-in function called `monotonically_increasing_id()`, designed to provide an integer ID that increases in value and is unique. These IDs are not necessarily sequential - there can be gaps, often quite large, between values.\r\n",
                "- Unlike a normal relational ID, Spark's is completely parallel - each partition is allocated up to 8 billion IDs that can be assigned. Notice that the ID fields in the sample table are integers, increasing in value, but are not sequential.\r\n",
                "-  It's a little out scope, but the IDs are a 64-bit number effectively split into groups based on the Spark partition. Each group contains 8.4 billion IDs, and there are 2.1 billion possible groups, none of which overlap.\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "20f0d4b8-4fc1-49dd-9737-75af24066c07"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Select all the unique council voters\r\n",
                "voter_df = df.select(df[\"VOTER NAME\"]).distinct()\r\n",
                "\r\n",
                "# Count the rows in voter_df\r\n",
                "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\r\n",
                "\r\n",
                "# Add a ROW_ID\r\n",
                "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\r\n",
                "\r\n",
                "# Show the rows with 10 highest IDs in the set\r\n",
                "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
            ],
            "metadata": {
                "azdata_cell_guid": "8dc1b6bd-b9ff-49d1-b86e-169b3f5d99e1"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Print the number of partitions in each DataFrame\r\n",
                "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())\r\n",
                "print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())\r\n",
                "\r\n",
                "# Add a ROW_ID field to each DataFrame\r\n",
                "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\r\n",
                "voter_df_single = voter_df_single.withColumn('ROW_ID', F.monotonically_increasing_id())\r\n",
                "\r\n",
                "# Show the top 10 IDs in each DataFrame \r\n",
                "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)\r\n",
                "voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)"
            ],
            "metadata": {
                "azdata_cell_guid": "29724a68-c69d-4e7e-b801-ccd9e3181d3c"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Determine the highest ROW_ID and save it in previous_max_ID\r\n",
                "previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]\r\n",
                "\r\n",
                "# Add a ROW_ID column to voter_df_april starting at the desired value\r\n",
                "voter_df_april = voter_df_april.withColumn('ROW_ID', previous_max_ID + F.monotonically_increasing_id())\r\n",
                "\r\n",
                "# Show the ROW_ID from both DataFrames and compare\r\n",
                "voter_df_march.select('ROW_ID').show()\r\n",
                "voter_df_april.select('ROW_ID').show()"
            ],
            "metadata": {
                "azdata_cell_guid": "69102e46-375c-458b-a792-91e74d46f0fe"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Caching\r\n",
                "\r\n",
                "- Caching in Spark refers to storing the results of a DataFrame in memory or on disk of the processing nodes in a cluster.\r\n",
                "- Caching improves the speed for subsequent transformations or actions as the data likely no longer needs to be retrieved from the original data source.\r\n",
                "- Using caching reduces the resource utilization of the cluster - there is less need to access the storage, networking, and CPU of the Spark nodes as the data is likely already present.\r\n",
                "\r\n",
                "## Disadvantages of caching\r\n",
                "- Very large data sets may not fit in the memory reserved for cached DataFrames.\r\n",
                "- Depending on the later transformations requested, the cache may not do anything to help performance. \r\n",
                "- If a data set does not stay cached in memory, it may be persisted to disk \r\n",
                "- Depending on the disk configuration of a Spark cluster, this may not be a large performance improvement\r\n",
                "- If you're reading from a local network resource and have slow local disk I/O, it may be better to avoid caching the objects.\r\n",
                "- Finally, the lifetime of a cached object is not guaranteed. Spark handles regenerating DataFrames for you automatically, but this can cause delays in processing.\r\n",
                "\r\n",
                "## Caching tips\r\n",
                "- Caching is incredibly useful, but only if you plan to use the DataFrame again. If you only need it for a single task, it's not worth caching. The best way to gauge performance with caching is to test various configurations. Try caching your DataFrames at various points in the processing cycle and check if it improves your processing time.\r\n",
                "- Try to cache in memory or fast NVMe / SSD storage. While still slower than main memory modern SSD based storage is drastically faster than spinning disk. Local spinning hard drives can still be useful if you are processing large DataFrames that require a lot of steps to generate, or must be accessed over the Internet.\r\n",
                "- Testing this is crucial. If normal caching doesn't seem to work, try creating intermediate Parquet representations. These can provide a checkpoint in case a job fails mid-task and can still be used with caching to further improve performance.\r\n",
                "- Finally, you can manually stop caching a DataFrame when you're finished with it. This frees up cache resources for other DataFrames.\r\n",
                "\r\n",
                "## Implementing Caching\r\n",
                "- Call `.cache()` on the DataFrame before Action \r\n",
                "- `.cache()` is a Spark transformation - nothing is actually cached until an action is called.\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "6224d58b-c6d6-4216-bea4-cf6cd9c80de6"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "voter_df = spark.read.csv('voter_data.txt.gz')\r\n",
                "voter_df.cache().count()\r\n",
                "\r\n",
                "voter_df = voter_df.withColumn('ID', monotonically_increasing_id())\r\n",
                "voter_df = voter_df.cache()\r\n",
                "voter_df.show()\r\n",
                "\r\n",
                "# Check `.is_cached` to determine cache status \r\n",
                "print(voter_df.is_cached)\r\n",
                "\r\n",
                "# Call.unpersist() when finished with DataFrame\r\n",
                "voter_df.unpersist()"
            ],
            "metadata": {
                "azdata_cell_guid": "1a0d0fe2-9867-48e4-883c-0eb7230f58c9"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Consider why the first run takes longer even though you've told it to cache() the DataFrame.\r\n",
                "# Remember that even though you've applied the caching transformation, it doesn't take effect until an action is run.\r\n",
                "# The action instantiates the caching after the distinct() function completes.\r\n",
                "# The second time, there is no need to recalculate anything so it returns almost immediately.\r\n",
                "\r\n",
                "start_time = time.time()\r\n",
                "\r\n",
                "# Add caching to the unique rows in departures_df\r\n",
                "departures_df = departures_df.distinct().cache()\r\n",
                "\r\n",
                "# Count the unique rows in departures_df, noting how long the operation takes\r\n",
                "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\r\n",
                "\r\n",
                "# Count the rows again, noting the variance in time of a cached DataFrame\r\n",
                "start_time = time.time()\r\n",
                "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "e3392c3f-fa7b-4590-90d2-10a8f15d3cbc"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Determine if departures_df is in the cache\r\n",
                "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\r\n",
                "print(\"Removing departures_df from cache\")\r\n",
                "\r\n",
                "# Remove departures_df from the cache\r\n",
                "departures_df.unpersist()\r\n",
                "\r\n",
                "# Check the cache status again\r\n",
                "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
            ],
            "metadata": {
                "azdata_cell_guid": "9e67bca8-34f8-42c9-9671-8a0bdf5c1dec"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Improve import performance\n",
                "\n",
                "- Spark clusters consist of two types of processes - one driver process and as many worker processes as required. \n",
                "- The driver handles task assignments and consolidation of the data results from the workers. The workers typically handle the actual transformation / action tasks of a Spark job.\n",
                "-  Once assigned tasks, they operate fairly independently and report results back to the driver. It is possible to have a single node Spark cluster but you'll rarely see this in a production environment. There are different ways to run Spark clusters - the method used depends on your specific environment.\n",
                "\n",
                "### Import Performance\n",
                "\n",
                "- When importing data to Spark DataFrames, it's important to understand how the cluster implements the job. The process varies depending on the type of task, but it's safe to assume that the more import objects available, the better the cluster can divvy up the job.\n",
                "- This may not matter on a single node cluster, but with a larger cluster each worker can take part in the import process. In clearer terms, `one large file will perform considerably worse than many smaller ones`.\n",
                "\n",
                "- Depending on the configuration of your cluster, you may not be able to process larger files, but could easily handle the same amount of data split between smaller files. Note you can define a single import statement, even if there are multiple files. You can use any form of standard wildcard symbol when defining the import filename. While less important, if objects are about the same size, the cluster will perform better than having a mix of very large and very small objects.\n",
                "\n",
                "### Defined Schemas\n",
                "\n",
                "Well-defined schemas in Spark drastically improve import performance. Without a schema defined, import tasks require reading the data multiple times to infer structure. This is very slow when you have a lot of data. Spark may not define the objects in the data the same as you would. Spark schemas also provide validation on import. This can save steps with data cleaning jobs and improve the overall processing time.\n",
                "\n",
                "### Splitting for Performance\n",
                "\n",
                "- There are various effective ways to split an object (files mostly) into more smaller objects. The first is to use built-in OS utilities such as split, cut, or awk. An example using split uses the -l argument with the number of lines to have per file (10000 in this case). The -d argument tells split to use numeric suffixes. The last two arguments are the name of the file to be split and the prefix to be used. Assuming 'largefile' has 10M records, we would have files named chunk-0000 through chunk-9999. \n",
                "\n",
                "\n",
                "- Another method is to use python (or any other language) to split the objects up as we see fit. Sometimes you may not have the tools available to split a large file. If you're going to be working with a DataFrame often, a simple method is to read in the single file then write it back out as parquet. We've done this in previous examples and it works well for later analysis even if the initial import is slow. It's important to note that if you're hitting limitations due to cluster sizing, try to do as little processing as possible before writing to parquet.\n",
                "\n",
                "- Note that in certain circumstances the results may be reversed. This is a side effect of running as a single node cluster. Depending on the tasks required and resources available, it may occasionally take longer than expected. If you perform multiple runs of the tasks, you should see the full file import as generally slower than the split file import\n",
                "\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "7fb08098-81ef-44ec-9a79-0e02b6f618d2"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Import the full and split files into DataFrames\r\n",
                "full_df = spark.read.csv('departures_full.txt.gz')\r\n",
                "split_df = spark.read.csv('departures_0*.txt.gz')\r\n",
                "\r\n",
                "# Print the count and run time for each DataFrame\r\n",
                "start_time_a = time.time()\r\n",
                "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\r\n",
                "print(\"Time to run: %f\" % (time.time() - start_time_a))\r\n",
                "\r\n",
                "start_time_b = time.time()\r\n",
                "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\r\n",
                "print(\"Time to run: %f\" % (time.time() - start_time_b))"
            ],
            "metadata": {
                "azdata_cell_guid": "89a1b342-12e8-4324-af0b-3e4b45dac9ff"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Â Configuration options\n",
                "\n",
                "Spark has many available configuration settings controlling all aspects of the installation. These configurations can be modified to best match the specific needs for the cluster. The configurations are available in the configuration files, via the Spark web interface, and via the run-time code.\n",
                "\n",
                "Reading configuration settings\n",
                "\n",
                "`spark.conf.get(<configuration name>)`\n",
                "\n",
                "Writing configuration settings\n",
                "\n",
                "`spark.conf.set(<configuration name>)`\n",
                "\n",
                "### Cluster Types\n",
                "\n",
                "1.  Single node clusters, deploying all components on a single system (physical / VM / container).\n",
                "2. Standalone clusters, with dedicated machines as the driver and workers. \n",
                "3. Managed clusters, meaning that the cluster components are handled by a third party cluster manager such as YARN, Mesos, or Kubernetes.\n",
                "\n",
                "### Driver\n",
                "\n",
                "- There is one driver per Spark cluster. The driver is responsible for several things, including the following: \n",
                "    1. Handling task assignment to the various nodes / processes in the cluster. \n",
                "    2. The driver monitors the state of all processes and tasks and handles any task retries. \n",
                "    3. The driver is also responsible for consolidating results from the other processes in the cluster. \n",
                "    4. The driver handles any access to shared data and verifies each worker process has the necessary resources (code, data, etc). \n",
                "\n",
                "- Doubling the memory compared to other nodes is recommended. This is useful for task monitoring and data consolidation tasks. \n",
                "- fast local storage is useful for running Spark in an ideal setup.\n",
                "\n",
                "### Workers\n",
                "\n",
                "- A Spark worker handles running tasks assigned by the driver and communicates those results back to the driver. Ideally, the worker has a copy of all code, data, and access to the necessary resources required to complete a given task. If any of these are unavailable, the worker must pause to obtain the resources. \n",
                "\n",
                "### Sizing a Cluster\n",
                "- More worker nodes is often better than larger nodes. This can be especially obvious during import and export operations as there are more machines available to do the work.\n",
                "\n",
                "- As with everything in Spark, test various configurations to find the correct balance for your workload. Assuming a cloud environment, 16 worker nodes may complete a job in an hour and cost $50 in resources. An 8 worker configuration might take 1.25 hrs but cost only half as much. \n",
                "\n",
                "- Finally, workers can make use of fast local storage (SSD / NVMe) for caching, intermediate files, etc.\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "94be8396-2827-49c2-a550-e4cdb87a5482"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Name of the Spark application instance\r\n",
                "app_name = spark.conf.get('spark.app.name')\r\n",
                "\r\n",
                "# Driver TCP port\r\n",
                "driver_tcp_port = spark.conf.get('spark.driver.port')\r\n",
                "\r\n",
                "# Number of join partitions\r\n",
                "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\r\n",
                "\r\n",
                "# Show the results\r\n",
                "print(\"Name: %s\" % app_name)\r\n",
                "print(\"Driver TCP port: %s\" % driver_tcp_port)\r\n",
                "print(\"Number of partitions: %s\" % num_partitions)"
            ],
            "metadata": {
                "azdata_cell_guid": "56676c50-ad1b-443e-beba-ddf848133fc7"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Store the number of partitions in variable\r\n",
                "before = departures_df.rdd.getNumPartitions()\r\n",
                "\r\n",
                "# Configure Spark to use 500 partitions\r\n",
                "spark.conf.set('spark.sql.shuffle.partitions', 500)\r\n",
                "\r\n",
                "# Recreate the DataFrame using the departures data file\r\n",
                "departures_df = spark.read.csv('departures.txt.gz').distinct()\r\n",
                "\r\n",
                "# Print the number of partitions for each instance\r\n",
                "print(\"Partition count before change: %d\" % before)\r\n",
                "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
            ],
            "metadata": {
                "azdata_cell_guid": "3ec46ea7-8aee-44fb-a819-9ddd0d214f0d"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Spark Performance Impprovements\r\n",
                "\r\n",
                "### Explain Plan\r\n",
                "\r\n",
                "- To understand performance implications of Spark, you must be able to see what it's doing under the hood. The easiest way to do this is to use the .explain() function on a DataFrame.The result is the estimated plan that will be run to generate results from the DataFrame.\r\n",
                "\r\n",
                "`voter_df = df.select(df['VOTER NAME']).distinct()`\r\n",
                "\r\n",
                "`voter_df.explain()`\r\n",
                "\r\n",
                "### Shuffling\r\n",
                "\r\n",
                "- Spark distributes data amongst the various nodes in the cluster. A side effect of this is what is known as shuffling. Shuffling is the moving of data fragments to various workers as required to complete certain tasks. \r\n",
                "- Shuffling is useful and hides overall complexity from the user. That being said, it can be slow to complete the necessary transfers, especially if a few nodes require all the data.\r\n",
                "- Shuffling lowers the overall throughput of the cluster as the workers must spend time waiting for the data to transfer. This limits the amount of available workers for the remaining tasks in the system. Shuffling is often a necessary component, but it's helpful to try to minimize it as much as possible.\r\n",
                "\r\n",
                "### How to Limit Shuffling\r\n",
                "\r\n",
                "- The DataFrame `.repartition()` function takes a single argument, the number of partitions requested. Repartitioning requires a full shuffle of data between nodes & processes and is quite costly. If you need to reduce the number of partitions, use the `.coalesce() `function instead. It takes a number of partitions smaller than the current one and consolidates the data without requiring a full data shuffle. Note: calling .coalesce() with a larger number of partitions does not actually do anything.\r\n",
                "\r\n",
                "- The `.join()` function is a great use of Spark and provides a lot of power. Calling .join() indiscriminately can often cause shuffle operations, leading to increased cluster load & slower processing times. To avoid some of the shuffle operations when joining Spark DataFrames you can use the `.broadcast()` function.\r\n",
                "\r\n",
                "- Broadcasting in Spark is a method to provide a copy of an object to each worker. When each worker has its own copy of the data, there is less need for communication between nodes. This limits data shuffles and it's more likely a node will fulfill tasks independently.\r\n",
                "\r\n",
                "- Using broadcasting can drastically speed up .join() operations, especially if one of the DataFrames being joined is much smaller than the other. \r\n",
                "\r\n",
                "- To implement broadcasting, you must import the broadcast function from pyspark.sql.functions. Once imported, simply call the broadcast function with the name of the DataFrame you wish to broadcast.\r\n",
                "\r\n",
                "- Note broadcasting can slow operations when using very small DataFrames or if you broadcast the larger DataFrame in a join. Spark will often optimize this for you, but as usual, run tests in your environment for best performance.\r\n",
                "\r\n",
                "`from pyspark.sql.functions import broadcast`\r\n",
                "\r\n",
                "`combined_df = df_1.join(broadcast(df_2))`\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "3efccab8-8a5a-4a42-90d6-e5dfbaa8b3f1"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Join the flights_df and aiports_df DataFrames\r\n",
                "normal_df = flights_df.join(airports_df, \\\r\n",
                "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\r\n",
                "\r\n",
                "# Show the query plan\r\n",
                "normal_df.explain()\r\n",
                "\r\n",
                "\r\n",
                "# Import the broadcast method from pyspark.sql.functions\r\n",
                "from pyspark.sql.functions import broadcast\r\n",
                "\r\n",
                "# Join the flights_df and airports_df DataFrames using broadcasting\r\n",
                "broadcast_df = flights_df.join(broadcast(airports_df), \\\r\n",
                "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\r\n",
                "\r\n",
                "# Show the query plan and compare against the original\r\n",
                "broadcast_df.explain()\r\n",
                "\r\n",
                "start_time = time.time()\r\n",
                "# Count the number of rows in the normal DataFrame\r\n",
                "normal_count = normal_df.count()\r\n",
                "normal_duration = time.time() - start_time\r\n",
                "\r\n",
                "start_time = time.time()\r\n",
                "# Count the number of rows in the broadcast DataFrame\r\n",
                "broadcast_count = broadcast_df.count()\r\n",
                "broadcast_duration = time.time() - start_time\r\n",
                "\r\n",
                "# Print the counts and the duration of the tests\r\n",
                "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\r\n",
                "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
            ],
            "metadata": {
                "azdata_cell_guid": "9afec751-fe3d-4b7a-bc31-f5f7c0f511ff"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Pipelines\r\n",
                "\r\n",
                "- Data pipelines are simply the set of steps needed to move from an input data source, or sources, and convert it to the desired output. \r\n",
                "\r\n",
                "- A data pipeline can consist of any number of steps or components, and can span many systems, a full production data pipeline will likely communicate with many systems.\r\n",
                "\r\n",
                "- A data pipeline typically consists of inputs, transformations, and the outputs of those steps. In addition, there is often validation and analysis steps before delivery of the data to the next user\r\n",
                "\r\n",
                "- In Spark, a data pipeline is not a formally defined object, but rather a concept.Different than if you've used the Pipeline object in Spark.ML\r\n",
                "\r\n",
                "What does a data pipeline look like ?\r\n",
                "\r\n",
                "1. Input(s)\r\n",
                " - CSV, JSON, webservices, databases\r\n",
                "2. Transformations \r\n",
                " - withColumn(), .filter(), .drop()\r\n",
                "3. Output(s)\r\n",
                " - CSV, Parquet, database\r\n",
                "4. Validation (idea is to run some form of testing on the data to verify it is as expected.)\r\n",
                "5. Analysis (often the final step before handing the data off to the next user. This can include things such as row counts, specific calculations, or pretty much anything that makes it easier for the user to consume the dataset.)\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "848da5e5-0eb7-427f-94e7-b1ae4f742acb"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Import the data to a DataFrame\r\n",
                "departures_df = spark.read.csv('2015-departures.csv.gz', header=True)\r\n",
                "\r\n",
                "# Remove any duration of 0\r\n",
                "departures_df = departures_df.filter(departures_df[3] > 0)\r\n",
                "\r\n",
                "# Add an ID column\r\n",
                "departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())\r\n",
                "\r\n",
                "# Write the file out to JSON format\r\n",
                "departures_df.write.json('output.json', mode='overwrite')\r\n",
                "\r\n",
                "# Order the dataframe with duration column\r\n",
                "departures_df = departures_df.withColumn('Duration', departures_df['Duration'].cast(IntegerType()))"
            ],
            "metadata": {
                "azdata_cell_guid": "259a309d-0fd5-4122-884b-aaf406fd2c86"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Data handling techniques\r\n",
                "\r\n",
                "- Spark's CSV parser can handle many common data issues via optional parameters. \r\n",
                " - Blank lines are automatically removed (unless specifically instructed otherwise) when using the CSV parsing. \r\n",
                "- Comments can be removed with an optional named argument, `comment`, and specifying the character that any comment line would be defined by. Note that this handles lines that begin with a specific comment. Parsing more complex comment usage requires more involved procedures. \r\n",
                "- Header rows can be parsed via an optional parameter named `header`, and set to 'True' or 'False'. If no schema is defined, column names will be initially set as defined by the header. If a schema is defined, the row is not used as data, but the header names are otherwise ignored.\r\n",
                "\r\n",
                "### Automatic column creation\r\n",
                "When importing CSV data into Spark, it will automatically create DataFrame columns if it can. It will split a row of text from the CSV on a defined separator argument named `sep`. If sep is not defined, it will default to using a comma.\r\n",
                "-  The CSV parser will still succeed in parsing data if the separator character is not within the string. It will store the entire row in a column named `_c0` by default. Using this trick allows parsing of nested or complex data."
            ],
            "metadata": {
                "azdata_cell_guid": "d3ac6a7a-e9ea-4112-af75-0e26709ff04c"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Import the file to a DataFrame and perform a row count\r\n",
                "annotations_df = spark.read.csv('annotations.csv.gz', sep='|')\r\n",
                "full_count = annotations_df.count()\r\n",
                "\r\n",
                "# Count the number of rows beginning with '#'\r\n",
                "comment_count = annotations_df.where(col('_c0').startswith('#')).count()\r\n",
                "\r\n",
                "# Import the file to a new DataFrame, without commented rows\r\n",
                "no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#')\r\n",
                "\r\n",
                "# Count the new DataFrame and verify the difference is as expected\r\n",
                "no_comments_count = no_comments_df.count()\r\n",
                "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, full_count - comment_count))split\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "76f3697b-7aae-4b36-885b-2997508ae542"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Split _c0 on the tab character and store the list in a variable\r\n",
                "tmp_fields = F.split(annotations_df['_c0'], '\\t')\r\n",
                "\r\n",
                "# Create the colcount column on the DataFrame\r\n",
                "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\r\n",
                "\r\n",
                "# Remove any rows containing fewer than 5 fields\r\n",
                "annotations_df_filtered = annotations_df.filter(~ (annotations_df[\"colcount\"] < 5))\r\n",
                "\r\n",
                "# Count the number of rows\r\n",
                "final_count = annotations_df_filtered.count()\r\n",
                "print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))"
            ],
            "metadata": {
                "azdata_cell_guid": "221a10f7-e3fd-4070-a3b8-d29dbe204325"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Split the content of _c0 on the tab character (aka, '\\t')\r\n",
                "split_cols = F.split(annotations_df['_c0'], '\\t')\r\n",
                "\r\n",
                "# Add the columns folder, filename, width, and height\r\n",
                "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\r\n",
                "split_df = split_df.withColumn('filename', split_cols.getItem(1))\r\n",
                "split_df = split_df.withColumn('width', split_cols.getItem(2))\r\n",
                "split_df = split_df.withColumn('height', split_cols.getItem(3))\r\n",
                "\r\n",
                "# Add split_cols as a column\r\n",
                "split_df = split_df.withColumn('split_cols', split_cols)"
            ],
            "metadata": {
                "azdata_cell_guid": "b6568cfc-1bcb-41f4-87c3-51f19e2890b3"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "def retriever(cols, colcount):\r\n",
                "  # Return a list of dog data\r\n",
                "  return cols[4:colcount]\r\n",
                "\r\n",
                "# Define the method as a UDF\r\n",
                "udfRetriever = F.udf(retriever, ArrayType(StringType()))\r\n",
                "\r\n",
                "# Create a new column using your UDF\r\n",
                "split_df = split_df.withColumn('dog_list', udfRetriever(split_df.split_cols, split_df.colcount))\r\n",
                "\r\n",
                "# Remove the original column, split_cols, and the colcount\r\n",
                "split_df = split_df.drop('_c0').drop('split_cols').drop('colcount')"
            ],
            "metadata": {
                "azdata_cell_guid": "f6069b66-ae5d-46c2-a77e-493164522e64"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Data validation\r\n",
                "\r\n",
                "- Validation is verifying that a dataset complies with an expected format. This can include verifying that the number of rows and columns is as expected. \r\n",
                "  - For example, is the row count within 2% of the previous month's row count? \r\n",
                "  - Another common test is do the data types match? If not specifically validated with a schema, does the content meet the requirements (only 9 characters or less, etc).\r\n",
                "  -  Finally, you can validate against more complex rules. This includes verifying that the values of a set of sensor readings are within physically possible quantities.\r\n",
                "\r\n",
                "- Validating via joins\r\n",
                " - One technique used to validate data in Spark is using joins to verify the content of a DataFrame matches a known set. Validating via a join will compare data against a set of known values. This could be a list of known ids, companies, addresses, etc. Joins make it easy to determine if data is present in a set. This could be only rows that are in one DataFrame, present in both, or present in neither.\r\n",
                " - Joins are also comparatively fast, especially vs validating individual rows against a long list of entries. The simplest example of this is using an inner join of two DataFrames to validate the data.\r\n",
                "\r\n",
                " `parsed_df = spark.read.parquet('parsed_data.parquet')`\r\n",
                " `company_df = spark.read.parquet('companies.parquet')`\r\n",
                " `verified_df = parsed_df.join(company_df, parsed_df.company == company_df.company)`\r\n",
                "\r\n",
                " - Complex rule validation is the idea of using Spark components to validate logic. This may be as simple as using the various Spark calculations to verify the number of columns in an irregular data set. The validation can also be applied against an external source: web service, local files, API calls. These rules are often implemented as a UDF to encapsulate the logic to one place and easily run against the content of a DataFrame. "
            ],
            "metadata": {
                "azdata_cell_guid": "49bc97c4-aa01-47c5-8008-0933b3e6404a"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Rename the column in valid_folders_df\r\n",
                "valid_folders_df = valid_folders_df.withColumnRenamed('_c0','folder')\r\n",
                "\r\n",
                "# Count the number of rows in split_df\r\n",
                "split_count = valid_folders_df.count()\r\n",
                "\r\n",
                "# Join the DataFrames\r\n",
                "joined_df = split_df.join(F.broadcast(valid_folders_df), \"folder\")\r\n",
                "\r\n",
                "# Compare the number of rows remaining\r\n",
                "joined_count = joined_df.count()\r\n",
                "print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))"
            ],
            "metadata": {
                "azdata_cell_guid": "fbb1638e-15b7-4459-b144-b71361152518"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Determine the row counts for each DataFrame\r\n",
                "split_count = split_df.count()\r\n",
                "joined_count = joined_df.count()\r\n",
                "\r\n",
                "# Create a DataFrame containing the invalid rows\r\n",
                "invalid_df = split_df.join(F.broadcast(joined_df), 'folder', 'left_anti')\r\n",
                "\r\n",
                "# Validate the count of the new DataFrame is as expected\r\n",
                "invalid_count = invalid_df.count()\r\n",
                "print(\" split_df:\\t%d\\n joined_df:\\t%d\\n invalid_df: \\t%d\" % (split_count, joined_count, invalid_count))\r\n",
                "\r\n",
                "# Determine the number of distinct folder rows removed\r\n",
                "invalid_folder_count = invalid_df.select('folder').distinct().count()\r\n",
                "print(\"%d distinct invalid folders found\" % invalid_folder_count)"
            ],
            "metadata": {
                "azdata_cell_guid": "d7d0d273-ced3-450f-b929-33405206774a"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Final analysis and delivery\r\n",
                "\r\n",
                "- Analysis calculations are the process of using the columns of data in a DataFrame to compute some useful value using Spark's functionality.\r\n",
                "\r\n",
                "- Spark UDFs are very powerful and flexible and are sometimes the only way to handle certain types of data. Unfortunately UDFs do come at a performance penalty compared to the built-in Spark functions, especially for certain operations. The solution is to perform calculations `inline` if possible.\r\n",
                "\r\n",
                "\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "898f4d97-6fef-4ad8-8f37-1f69e1eb6b32"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def getAvgSale(saleslist):\r\n",
                "      totalsales = 0\r\n",
                "      count = 0\r\n",
                "      for sale in saleslist:\r\n",
                "          totalsales += sale[2] + sale[3]\r\n",
                "          count += 2\r\n",
                "      return totalsales / count\r\n",
                "\r\n",
                "udfGetAvgSale = udf(getAvgSale, DoubleType())\r\n",
                "df = df.withColumn('avg_sale', udfGetAvgSale(df.sales_list))\r\n",
                "\r\n",
                "# Inline Calculations\r\n",
                "df = df.read.csv('datafile')\r\n",
                "df = df.withColumn('avg', (df.total_sales / df.sales_count))\r\n",
                "df = df.withColumn('sq_ft', df.width * df.length)\r\n",
                "df = df.withColumn('total_avg_size', udfComputeTotal(df.entries) / df.numEntries)\r\n",
                "\r\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "5f78386b-257d-4a1a-9d56-863c1901ba24"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Select the dog details and show 10 untruncated rows\r\n",
                "print(joined_df.select('dog_list').show(10, truncate=False))\r\n",
                "\r\n",
                "# Define a schema type for the details in the dog list\r\n",
                "DogType = StructType([\r\n",
                "\tStructField(\"breed\", StringType(), False),\r\n",
                "    StructField(\"start_x\", IntegerType(), False),\r\n",
                "    StructField(\"start_y\", IntegerType(), False),\r\n",
                "    StructField(\"end_x\", IntegerType(), False),\r\n",
                "    StructField(\"end_y\", IntegerType(), False)\r\n",
                "])"
            ],
            "metadata": {
                "azdata_cell_guid": "64f1172d-284a-450b-85db-1580994a87eb"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Create a function to return the number and type of dogs as a tuple\r\n",
                "def dogParse(doglist):\r\n",
                "  dogs = []\r\n",
                "  for dog in doglist:\r\n",
                "    (breed, start_x, start_y, end_x, end_y) = dog.split(',')\r\n",
                "    dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))\r\n",
                "  return dogs\r\n",
                "\r\n",
                "# Create a UDF\r\n",
                "udfDogParse = F.udf(dogParse, ArrayType(DogType))\r\n",
                "\r\n",
                "# Use the UDF to list of dogs and drop the old column\r\n",
                "joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')\r\n",
                "\r\n",
                "# Show the number of dogs in the first 10 rows\r\n",
                "joined_df.select(F.size('dogs')).show(10)"
            ],
            "metadata": {
                "azdata_cell_guid": "66a46c50-6a12-4869-a063-cc262c1acdf9"
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "code",
            "source": [
                "# Define a UDF to determine the number of pixels per image\r\n",
                "def dogPixelCount(doglist):\r\n",
                "  totalpixels = 0\r\n",
                "  for dog in doglist:\r\n",
                "    totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])\r\n",
                "  return totalpixels\r\n",
                "\r\n",
                "# Define a UDF for the pixel count\r\n",
                "udfDogPixelCount = F.udf(dogPixelCount, IntegerType())\r\n",
                "joined_df = joined_df.withColumn('dog_pixels', udfDogPixelCount('dogs'))\r\n",
                "\r\n",
                "# Create a column representing the percentage of pixels\r\n",
                "joined_df = joined_df.withColumn('dog_percent', (joined_df.dog_pixels / (joined_df.width * joined_df.height)) * 100)\r\n",
                "\r\n",
                "# Show the first 10 annotations with more than 60% dog\r\n",
                "joined_df.where('dog_percent > 60').show(10)"
            ],
            "metadata": {
                "azdata_cell_guid": "fbd830ca-afc4-4897-8ffe-a3b5866885e3"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}